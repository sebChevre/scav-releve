{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cairo\n",
    "from datetime import datetime\n",
    "import difflib\n",
    "import extract_msg # conda install -c conda-forge msg-extractor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd # conda install -c anaconda xlrd for read_excel\n",
    "from pdf2image import convert_from_path # poppler-utils has to be installed along\n",
    "from PIL import Image\n",
    "import PyPDF2\n",
    "import pytesseract # install tesseract & pytesseract both\n",
    "import re\n",
    "import string\n",
    "import subprocess as sp\n",
    "import tabula\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAB_dict = {1: \"RuferLab\", 2: \"SCITEC\", 3: \"Viteos\", 4: \"ABL Analytics\", 5: \"RWB analub\", -1: \"!!! UNKNOWN !!!\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_Analysis = [\"ID\", \"ParamID\", \"EchantillonID\", \"Valeur\", \"AnalysisTime\"]\n",
    "col_Parameter = [\"ID\", \"Nom\", \"Group\", \"Unit\", \"Limit\"]\n",
    "col_Echantillon = [\"ID\", \"LocationID\", \"personID\", \"Code\", \"FloconsNb\", \"SamplingMethod\", \"Temperature\"]\n",
    "col_Location = [\"ID\", \"Address\", \"WaterType\", \"Treatment\", \"Village\", \"Commune\", \"Distributeur\"]#, \"ParentLocID\"] # for now do not take into account\n",
    "col_Sampler = [\"ID\", \"SamplerName\"]\n",
    "\n",
    "col_perAnalysis = col_Analysis[3:] + col_Parameter[1:]\n",
    "col_perEchantillon = col_Echantillon[3:] + col_Location[1:-1] + col_Sampler[1:]\n",
    "col_All = col_perAnalysis + col_perEchantillon\n",
    "\n",
    "col_All"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: input_str : input string\n",
    "# Output: final_str : input string without any accents\n",
    "# https://stackoverflow.com/questions/517923/what-is-the-best-way-to-remove-accents-normalize-in-a-python-unicode-string\n",
    "def remove_accents(input_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    only_ascii = nfkd_form.encode('ASCII', 'ignore')\n",
    "    final_str = only_ascii.decode('utf-8')\n",
    "    return final_str\n",
    "\n",
    "\n",
    "# Input: fp : filepath\n",
    "#        page_nb : page_nb-th page to be visualized\n",
    "def visualize_pdf(fp, page_nb=-1):\n",
    "    doc = convert_from_path(fp)\n",
    "    if page_nb >= 0:\n",
    "        display(doc[page_nb])\n",
    "    else:\n",
    "        for d in doc:\n",
    "            display(d)\n",
    "    \n",
    "    return\n",
    "\n",
    "# Input: fp : filepath\n",
    "#        page_nb : text extracted from page_nb-th page with pytesseract\n",
    "# Output: texts : list of text (per line)\n",
    "def pytesseract_text(fp, page_nb=-1):\n",
    "    doc = convert_from_path(fp)\n",
    "    \n",
    "    if page_nb >= 0:\n",
    "        return list(pytesseract.image_to_string(doc[page_nb], lang='fra').split('\\n'))\n",
    "    else:\n",
    "        texts = []\n",
    "        for i in range(len(doc)):\n",
    "            texts.append(list(pytesseract.image_to_string(doc[i], lang='fra').split('\\n')))\n",
    "\n",
    "        return texts\n",
    "    \n",
    "# Input: fp : filepath\n",
    "# Output: pgNb : number of pages that the pdf contains\n",
    "def get_nb_pages(fp):\n",
    "    # https://stackoverflow.com/questions/49939085/xref-table-not-zero-indexed-id-numbers-for-objects-will-be-corrected-wont-con\n",
    "    reader = PyPDF2.PdfFileReader(open(fp, mode='rb'), strict=False)\n",
    "    pgNb = reader.getNumPages()\n",
    "    \n",
    "    return pgNb\n",
    "\n",
    "\n",
    "# Input: fp : filepath\n",
    "#        page_nb : -1 for 'all' else specified number of page on which the tables are extracted\n",
    "# Output: tables: list of table extracted from the specified number of page (or the entire pdf)\n",
    "def get_table(fp, page_nb=-1, multiple_tables=True, guess=True, output_format=None):\n",
    "    # Extract 1 page\n",
    "    if page_nb >= 0:\n",
    "        tables = tabula.read_pdf(fp, pages=page_nb+1, stream=False, multiple_tables=multiple_tables, guess=guess, output_format=output_format)\n",
    "        \n",
    "    # Extract all the pages\n",
    "    else:\n",
    "        tables = tabula.read_pdf(fp, pages='all', stream=True, multiple_tables=multiple_tables, guess=guess, output_format=output_format)\n",
    "    \n",
    "    return tables\n",
    "\n",
    "def extract_table(fp):\n",
    "    pg_Nb = get_nb_pages(fp)\n",
    "    dfs = get_table(fp, guess=True)\n",
    "    \n",
    "    # Case where one of the pages' tables are not correctly extracted\n",
    "    if len(dfs) < pg_Nb:\n",
    "        new_dfs = []\n",
    "        for i in range(pg_Nb):\n",
    "            tables = get_table(fp, page_nb=i, guess=True)\n",
    "            if len(tables) == 0:\n",
    "                tables = get_table(fp, page_nb=i, guess=False)\n",
    "                if len(tables) == 0:\n",
    "                    print(\"NO TABLES EXTRACTED FROM FILE : \", fp)\n",
    "            new_dfs += tables\n",
    "    \n",
    "        if len(new_dfs) > len(dfs):\n",
    "            return new_dfs\n",
    "\n",
    "    return dfs\n",
    "\n",
    "\n",
    "# Input: fp : filepath\n",
    "#        page_nb : -1 for 'all' else specified number of page on which the tables are extracted\n",
    "# Output: bboxes: list of bbox extracted from each detected table\n",
    "def get_bbox_table(fp, page_nb=-1):\n",
    "    # Only for getting the table bounding box purpose\n",
    "    # https://blog.chezo.uno/a-recent-update-of-tabula-py-a923d2ab667b\n",
    "    tables = get_table(fp, page_nb, output_format=\"json\")\n",
    "    bboxes = []\n",
    "    for table in tables:\n",
    "        top = table['top']\n",
    "        left = table['left']\n",
    "        bottom = table['height'] + top\n",
    "        right = table['width'] + left\n",
    "        bbox = [top, left, bottom, right]\n",
    "        bboxes.append(bbox)\n",
    "    \n",
    "    return bboxes\n",
    "    \n",
    "# Add to a dictionary of type {k: [v]}\n",
    "# k: key to which v has to be appended\n",
    "# v: append to the list of the given k or create a new list [v] (if v is not already a list)\n",
    "def add_item_to_dict_of_list(d, k, v):\n",
    "    if k in d.keys():\n",
    "        if isinstance(v, list):\n",
    "            d[k] += v\n",
    "        else:\n",
    "            d[k].append(v)\n",
    "    else:\n",
    "        if isinstance(v, list):\n",
    "            d[k] = v\n",
    "        else:\n",
    "            d[k] = [v]\n",
    "        \n",
    "    return d\n",
    "\n",
    "\n",
    "def merge_two_columns_at_row(df, first_col_idx, second_col_idx, row_idx):\n",
    "    first_val = df.at[row_idx, df.columns[first_col_idx]]\n",
    "    second_val = df.at[row_idx, df.columns[second_col_idx]]\n",
    "        \n",
    "    if first_val is np.nan and second_val is np.nan:\n",
    "        new_val = np.nan\n",
    "        \n",
    "    elif first_val is np.nan:\n",
    "        new_val = second_val\n",
    "    elif second_val is np.nan:\n",
    "        new_val = first_val\n",
    "    else:\n",
    "        new_val = str(first_val) + \" \" + str(second_val)\n",
    "    #if type(new_val) == str:\n",
    "    #    df[df.columns[first_col_idx]] = df[df.columns[first_col_idx]].astype('str')\n",
    "    df.at[row_idx, df.columns[first_col_idx]] = new_val\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_two_columns(df, first_col_idx, second_col_idx):\n",
    "    for i in range(len(df)):\n",
    "        df = merge_two_columns_at_row(df, first_col_idx, second_col_idx, i)\n",
    "    \n",
    "    df.drop(columns=df.columns[second_col_idx], axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def is_unnamed(c):\n",
    "    return str(c).startswith(\"Unnamed\")\n",
    "\n",
    "\n",
    "def unify_NaN(df):\n",
    "    nan_values = ['N/A', 'n/a']\n",
    "    return df.replace(nan_values, np.nan)\n",
    "\n",
    "\n",
    "# Use convert_from_path to check if it is a pdf file or not (ex) lab's logo img file)\n",
    "def is_pdf(attachment):\n",
    "    tmp = open(\"tmp.pdf\", 'wb')\n",
    "    tmp.write(attachment.data)\n",
    "    tmp.close()\n",
    "    \n",
    "    out = True\n",
    "    try:\n",
    "        doc = convert_from_path(\"tmp.pdf\")\n",
    "    except Exception:\n",
    "        out = False\n",
    "        \n",
    "    os.remove(\"tmp.pdf\")\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "# MSG file extractor\n",
    "# Note that sometimes, there exists msg files + respective pdf files -> Do not extract if pdf exists already\n",
    "def extract_pdf_from_msg(filepath):\n",
    "    print(filepath)\n",
    "    \n",
    "    # Skip wif the pdf file exists already\n",
    "    if os.path.isfile(filepath[:-3]+\"pdf\"):\n",
    "        return None\n",
    "\n",
    "    filenames = []\n",
    "\n",
    "    msg = extract_msg.Message(filepath)\n",
    "    attachments = msg.attachments\n",
    "    \n",
    "    attachments = [att for att in attachments if (is_pdf(att))]\n",
    "    if len(attachments) == 0:\n",
    "        print(\"NO ATTACHMENT\")\n",
    "        return None\n",
    "    \n",
    "    for i, attachment in enumerate(attachments):\n",
    "        filename = \".\".join(filepath.split('.')[:-1])\n",
    "        print(filename)\n",
    "        if i>0 : \n",
    "            filename += \"_\"+str(i+1)\n",
    "        file = open(filename+\".pdf\", 'wb')\n",
    "        print(\"EXTRACT : \", filename)\n",
    "        file.write(attachment.data)\n",
    "        file.close()\n",
    "        filenames.append(filename+\".pdf\")\n",
    "        \n",
    "    if len(filenames) > 1:\n",
    "        print(\"ERROR : MORE THAN ONE PDF EXTRACTED FROM MSG FILE\")\n",
    "        print(filenames)\n",
    "    \n",
    "    return filenames[0]\n",
    "\n",
    "\n",
    "# Check whether the given pdf is iamge-based or not searchable pdf\n",
    "# https://stackoverflow.com/questions/55704218/how-to-check-if-pdf-is-scanned-image-or-contains-text\n",
    "def check_image_based_pdf(fp):\n",
    "    image_based = True\n",
    "    \n",
    "    output = sp.getoutput(\"ocrmypdf \" + fp + \" output.pdf\")\n",
    "    if not re.search(\"PriorOcrFoundError: page already has text!\",output):\n",
    "        image_based = True\n",
    "        print(\"Uploaded scanned pdf\")\n",
    "    else:\n",
    "        image_based = False\n",
    "        print(\"Uploaded digital pdf\")\n",
    "    \n",
    "    return image_based\n",
    "\n",
    "def from_image_based_to_searchable_pdf(fp):\n",
    "    !ocrmypdf $fp $fp --remove-background --force-ocr\n",
    "    \n",
    "    return fp\n",
    "\n",
    "\n",
    "def header_to_row(df):\n",
    "    return df.reset_index(drop=True).T.reset_index().T.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Autocontrole_PATH = \"./Data/Autocontrole_EP/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sanitize the directory and file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_name_files(parent_dir):\n",
    "    sub = [x for x in os.listdir(parent_dir) if not x.startswith(\".\")]\n",
    "    \n",
    "    if len(sub) == 0:\n",
    "        return\n",
    "    \n",
    "    for c in sub:\n",
    "        new_c = remove_accents(c.replace(\" \", \"_\").replace(string.punctuation, \"\")).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "        #print(c)\n",
    "        \n",
    "        os.rename(parent_dir + c, parent_dir + new_c)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sanitize_name_files(Autocontrole_PATH)\n",
    "for tup in os.walk(Autocontrole_PATH):\n",
    "    if len(tup[1]) > 0:\n",
    "        for ts in tup[1]:\n",
    "            sanitize_name_files(tup[0] +\"/\" + ts + \"/\")\n",
    "            for tt in os.walk(ts):\n",
    "                if len(tt[1] > 0):\n",
    "                    for ttt in tt[1]:\n",
    "                        for tttt in os.walk(ttt):\n",
    "                            sanitize_name_files(tup[0] +\"/\"+ ts + \"/\" + ttt + \"/\" + tttt + \"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. create the pdf file for all the msg files & classify each document to its respective laboratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_keyword(kw, txt):\n",
    "    return np.any([True if kw in t.lower() else False for t in txt])\n",
    "\n",
    "def get_laboratory(file):\n",
    "    txt = pytesseract_text(file, page_nb=0)\n",
    "    \n",
    "    if check_keyword(\"ruferlab\", txt):\n",
    "        LAB = 1\n",
    "    # To validate the keyword to check\n",
    "    elif check_keyword(\"scitec\", txt):\n",
    "        LAB = 2\n",
    "    elif check_keyword(\"viteos\", txt):\n",
    "        LAB = 3\n",
    "    elif check_keyword(\"abl analytics\", txt):\n",
    "        LAB = 4\n",
    "    elif check_keyword(\"rwb analub\", txt):\n",
    "        LAB = 5\n",
    "    else:\n",
    "        LAB = -1\n",
    "        \n",
    "    return LAB"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#CODE to activate in order to classify the text files into different laboratories\n",
    "RUFERLAB_FILES = []\n",
    "SCITEC_FILES = []\n",
    "VITEOS_FILES = []\n",
    "ABL_RWB_FILES = []\n",
    "\n",
    "LAB_dict = {1: \"RuferLab\", 2: \"SCITEC\", 3: \"Viteos\", 4: \"ABL Analytics\", 5: \"RWB analub\", -1: \"!!! UNKNOWN !!!\"}\n",
    "\n",
    "communes = [x for x in os.listdir(Autocontrole_PATH) if not x.startswith(\".\")]\n",
    "\n",
    "for commune in communes:\n",
    "    print(\"COMMUNE : \", commune)\n",
    "    # 3-tuple : (name of the directory, list of subdirectories, list of files)\n",
    "    # Leaf-side directory : check tuple[1] is empty -> get list of files, appending the path tuple[0]+tuple[2]\n",
    "    for tup in os.walk(Autocontrole_PATH+commune):\n",
    "        subfiles = [tup[0]+\"/\"+x for x in tup[2] if any([x.endswith(extension) for extension in [\".pdf\", \".xlsx\", \".msg\"]])]\n",
    "        for file in subfiles:\n",
    "            print(file)\n",
    "            #if not np.any([file.replace(\".msg\", \".pdf\") in ls for ls in FILES]):\n",
    "            if file.endswith(\".msg\"):\n",
    "                tmp = extract_pdf_from_msg(file)\n",
    "                if tmp:\n",
    "                    file = tmp\n",
    "\n",
    "            if file.endswith(\".pdf\"):\n",
    "                pgNb = pgNb = get_nb_pages(file)\n",
    "\n",
    "                file_lab = get_laboratory(file)\n",
    "                if file_lab == 1:\n",
    "                    RUFERLAB_FILES.append(file)\n",
    "                elif file_lab == 2:\n",
    "                    SCITEC_FILES.append(file)\n",
    "                elif file_lab == 3:\n",
    "                    VITEOS_FILES.append(file)\n",
    "                elif file_lab == 4 or file_lab == 5:\n",
    "                    ABL_RWB_FILES.append(file)\n",
    "                print(LAB_dict[file_lab])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#CODE to activate in order to write the classified files into a text file\n",
    "FILES = [RUFERLAB_FILES, SCITEC_FILES, VITEOS_FILES, ABL_RWB_FILES]\n",
    "fns = [\"ruferlab_files.txt\", \"scitec_files.txt\", \"viteos_files.txt\", \"ablrwb_files.txt\"]\n",
    "\n",
    "for fn, ls in zip(fns, FILES):\n",
    "    textfile = open(fn, \"w\")\n",
    "    for l in ls:\n",
    "        textfile.write(l + \"\\n\")\n",
    "    textfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE to read a previously written text file containing the classified files list\n",
    "RUFERLAB_FILES = []\n",
    "SCITEC_FILES = []\n",
    "VITEOS_FILES = []\n",
    "ABL_RWB_FILES = []\n",
    "\n",
    "FILES = [RUFERLAB_FILES, SCITEC_FILES, VITEOS_FILES, ABL_RWB_FILES]\n",
    "fns = [\"ruferlab_files.txt\", \"scitec_files.txt\", \"viteos_files.txt\", \"ablrwb_files.txt\"]\n",
    "\n",
    "for fn, ls in zip(fns, FILES):\n",
    "    textfile = open(fn, \"r\")\n",
    "    for l in textfile:\n",
    "        ls.append(l.replace(\"\\n\", \"\"))\n",
    "    textfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data extraction per laboratory type\n",
    "\n",
    "General methods applicable to both laboratories (ABL_RWB_LAB and RUFER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sampler(text_ls, LAB):\n",
    "    sampler_name = \"\"\n",
    "    \n",
    "    if LAB == 1 or LAB == 3:\n",
    "        text_ls = [y for x in text_ls for y in x if y.strip() != '']\n",
    "        line = [t for t in text_ls if \"Nom du préleveur\" in t or \"préleveur\" in t or \"Nom du preleveur\" in t]\n",
    "        if len(line) > 0 :\n",
    "            line = line[0]\n",
    "            sampler_name = line.replace('Nom du préleveur', '').replace(\"Nom du preleveur\", '').replace(\"préleveur\", '').strip()\n",
    "        \n",
    "    if LAB == 4 or LAB == 5:\n",
    "        # preprocess the text_ls\n",
    "        text_ls = [x for x in text_ls if x.strip() != '']\n",
    "        sampler_keyword = 'Prélevé '\n",
    "        line = [t for t in text_ls if sampler_keyword in t][0].replace(sampler_keyword, \"\")\n",
    "        if \" le\" in line:\n",
    "            sampler_name = line[:line.index(\" le\")]\n",
    "    \n",
    "    if len(sampler_name.split()) < 2:\n",
    "        return sampler_name\n",
    "    \n",
    "    sampler_name = sampler_name.split()\n",
    "    firstname = sampler_name[0]\n",
    "    lastname = sampler_name[1]\n",
    "\n",
    "    #return lastname, firstname\n",
    "    return lastname + \" \" + firstname\n",
    "\n",
    "\n",
    "def extract_watertype(texts, LAB):\n",
    "    watertype = \"\"\n",
    "    \n",
    "    watertype_keyword_dict = {1: \"Nature de l'échantillon\", 2: \"\", 3: \"\", 4: \"Nature de l'échantillon: \", 5: \"Nature de l'échantillon: \"}\n",
    "    \n",
    "    watertype_keyword = watertype_keyword_dict[LAB]\n",
    "    line = [t for t in texts if watertype_keyword in t]\n",
    "    if len(line) == 0:\n",
    "        print(\"extract_watertype(): Water Type Unfound\")\n",
    "        return None\n",
    "    \n",
    "    if LAB == 1:\n",
    "        watertype = line[0].replace(watertype_keyword, '')\n",
    "        \n",
    "    if LAB == 4 or LAB == 5:\n",
    "        watertype = line[0].replace(watertype_keyword, '')\n",
    "    \n",
    "    return watertype.strip()\n",
    "\n",
    "\n",
    "def extract_date(texts, LAB=1):\n",
    "    date = \"\"\n",
    "    \n",
    "    line = [t for t in texts if \"\" in t or \"Date d'échantillonnage\" in t]\n",
    "    if len(line) == 0:\n",
    "        print(\"extract_date(): Date Unfound\")\n",
    "        return None\n",
    "    \n",
    "    if \"Date de prélèvement\" in line[0]:\n",
    "        date = line[0].replace(\"Date de prélèvement\", '')\n",
    "    elif \"Date d'échantillonnage\" in line[0]:\n",
    "        date = line[0].replace(\"Date d'échantillonnage\", '')\n",
    "    \n",
    "    return date.strip()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LAB : ABL_RWB_LAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_table_keyword(df, start=\"Analyses effectuees\", end=[\"Commentaire:\", \"Toute reproduction\"]):\n",
    "    if start in df.values:\n",
    "        start_idx = df[df == start].dropna(how='all').index[0]+1\n",
    "        \n",
    "        if end[0] in df.values:\n",
    "            end_idx = df[df == end[0]].dropna(how='all').index[0]\n",
    "            df = df[start_idx:end_idx].copy()\n",
    "        # Check if Toute reproduction keyword is contained in one of the substring of df\n",
    "        elif np.any([True for z in [y for x in df.values for y in x if not y is np.nan] if end[1] in str(z)]):\n",
    "            \n",
    "            end_idx = df.replace(np.nan, \"\").applymap(lambda x: end[1] in x).replace(False, np.nan).dropna(how='all').index[0]\n",
    "            df = df[start_idx:end_idx].copy()\n",
    "        else:\n",
    "            df = df[start_idx:].copy()\n",
    "            \n",
    "        df.dropna(axis=1, how='all', inplace=True)\n",
    "        df.columns = df.iloc[0].values\n",
    "        df.drop(df.index[0], axis=0, inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "    return df\n",
    "    \n",
    "\n",
    "def split_merged_columns(df, col, col_idx, col_name, keywords_dict):\n",
    "    def get_common_key(s, ks, start=True):\n",
    "        # replace any multiple whitespaces with single whitespace\n",
    "        s = \" \".join(s.split())\n",
    "        for k in ks:\n",
    "            if k in s:\n",
    "                if start:\n",
    "                    return k\n",
    "                else:\n",
    "                    return s.replace(k, \"\").strip()\n",
    "            \n",
    "        if start:\n",
    "            return s.split()[0]\n",
    "        else:\n",
    "            return ' '.join(s.split()[1:])\n",
    "    \n",
    "    if col_name in keywords_dict.keys():\n",
    "        keywords = keywords_dict[col_name]\n",
    "        \n",
    "        start_split = lambda x: get_common_key(x, keywords)\n",
    "        end_split = lambda x: get_common_key(x, keywords, start=False)\n",
    "    else:\n",
    "        start_split = lambda x: x.split()[0]\n",
    "        end_split = lambda x: ' '.join(x.split()[1:])\n",
    "    # start split\n",
    "    df.insert(col_idx, col_name, df[df.columns[col_idx]].map(start_split).values)\n",
    "    # end_split\n",
    "    df[col] = df[col].map(end_split)\n",
    "    df = df.applymap(lambda x: np.nan if x == \"\" else x)\n",
    "    df.rename({col: col.replace(col_name, \"\").strip()}, axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def format_ABL_RWB_df(df, cols):\n",
    "    cols = [\"Parametre\", \"Methode\", \"Date\", \"Resultat\", \"Unite\"]\n",
    "    col_keywords_dict = {\"Parametre\": [\"Temperature mesuree in-situ\", \"Escherichia coli\", \"Enterocoques\", \"Germes aerobies\", \"Conductivite (25 C)\", \"Absorption UV 254\"]}\n",
    "    \n",
    "    # Case where the result column has been split into two (because of < or <=)\n",
    "    if len(df.columns) == len(cols)+1 and np.all([x in [\"<\", \"<=\", \">\", \">=\", \"~\"]for x in df[df.columns[3]].dropna().unique().tolist()]):\n",
    "        df = merge_two_columns(df, 3, 4)\n",
    "    \n",
    "    # When we suppose the table is correctly extracted but without correct column names -> add the cols\n",
    "    if len(df.columns) == len(cols) and not np.any([x in cols for x in df.columns.values]):\n",
    "        # Swap eventual unnmaed column to NaN\n",
    "        df.rename({c: np.nan for c in df.columns if is_unnamed(c)}, axis=1, inplace=True)\n",
    "        # Shift the header to the row\n",
    "        # https://stackoverflow.com/questions/53282075/turn-the-column-headers-into-the-first-row-and-row-headers-into-the-first-column\n",
    "        df = df.reset_index(drop=True).T.reset_index().T\n",
    "        df.columns = cols\n",
    "        \n",
    "        return df.reset_index(drop=True)\n",
    "        \n",
    "    # Create correct columns\n",
    "    if len(df.columns) == len(cols) and np.equal(df.columns, cols).all():\n",
    "        return df\n",
    "    else:\n",
    "        col_idx = 0\n",
    "        for i, c in enumerate(cols):\n",
    "            # Process NaN column : merge it\n",
    "            # Then work with the given column name as this merge has not happened\n",
    "            if df.columns[col_idx] is np.nan:\n",
    "                df = merge_two_columns(df, col_idx-1, col_idx)\n",
    "\n",
    "            if df.columns[col_idx] == c:\n",
    "                col_idx += 1\n",
    "\n",
    "            elif c in df.columns[col_idx]:\n",
    "                df = split_merged_columns(df, df.columns[col_idx], col_idx, c, col_keywords_dict)\n",
    "                if c == df.columns[col_idx].split()[-1]:\n",
    "                    col_idx += 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_table_from_ABL_RWB_pdf(fp, debug=False, continuous_table=False):\n",
    "    out_tables = []\n",
    "    \n",
    "    pg_Nb = get_nb_pages(fp)\n",
    "    \n",
    "    if debug:\n",
    "        print(fp)\n",
    "        print(pg_Nb)\n",
    "        visualize_pdf(fp)\n",
    "\n",
    "    dfs = get_table(fp, guess=True)\n",
    "    \n",
    "    # Case where one of the pages' tables are not correctly extracted\n",
    "    if len(dfs) < pg_Nb:\n",
    "        dfs = []\n",
    "        for i in range(pg_Nb):\n",
    "            tables = get_table(fp, page_nb=i, guess=True)\n",
    "            if len(tables) == 0:\n",
    "                tables = get_table(fp, page_nb=i, guess=False)\n",
    "                if len(tables) == 0:\n",
    "                    print(\"NO TABLES EXTRACTED FROM FILE : \", fp)\n",
    "            dfs += tables\n",
    "\n",
    "    for df in dfs:\n",
    "        df = df.copy()\n",
    "        df = df.applymap(lambda x: remove_accents(x) if isinstance(x, str) else x)\n",
    "        \n",
    "        ABL_RWB_Columns = [\"Parametre\", \"Methode\", \"Date\", \"Resultat\", \"Unite\"]\n",
    "        \n",
    "        df = filter_table_keyword(df)\n",
    "        if debug:\n",
    "            print(\"After Filter\")\n",
    "            display(df)\n",
    "        df = format_ABL_RWB_df(df, cols=ABL_RWB_Columns)\n",
    "        if debug:\n",
    "            print(\"Final\")\n",
    "            display(df)\n",
    "        out_tables.append(df)\n",
    "        \n",
    "    if np.all([np.equal(tab.columns, out_tables[0].columns).all() for tab in out_tables]):\n",
    "        out = pd.concat(out_tables).reset_index(drop=True)\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def extractor_ABL_RWB(filepath):\n",
    "    LAB = 4 # As ABL_RWB\n",
    "    \n",
    "    table = extract_table_from_ABL_RWB_pdf(filepath)\n",
    "    # Handle the case if the pdf is image based\n",
    "    if len(table) == 0:\n",
    "        if check_image_based_pdf(filepath):\n",
    "            filepath = from_image_based_to_searchable_pdf(filepath)\n",
    "            table = extract_table_from_ABL_RWB_pdf(filepath)\n",
    "\n",
    "    text_ls = pytesseract_text(filepath)[0]\n",
    "    texts = [x for x in text_ls if x.strip() != '']\n",
    "    \n",
    "    echantillon_code = extract_echantillon_code(texts, LAB)\n",
    "    flocons_nb = extract_flocons_nb(texts, LAB)\n",
    "    temperature = table[table.Parametre.str.contains(\"Température\")].Resultat.values\n",
    "    temperature = temperature[0] if len(temperature) > 0 else None\n",
    "    address = extract_address(texts, LAB)\n",
    "    watertype = extract_watertype(texts, LAB)\n",
    "    treatment = extract_treatment(texts, LAB)\n",
    "    sampler = extract_sampler(texts, LAB)\n",
    "    \n",
    "    print(echantillon_code, flocons_nb, temperature, address, watertype, treatment, sampler)\n",
    "    \n",
    "    table.drop(columns=[\"Methode\"], inplace=True)\n",
    "    table.rename(columns={\"Resultat\": \"Value\"}, inplace=True)\n",
    "    \n",
    "    table[\"Limit\"] = None\n",
    "    table[\"Code\"] = echantillon_code\n",
    "    table[\"FloconsNb\"] = flocons_nb\n",
    "    table[\"Temperature\"] = temperature\n",
    "    table[\"Address\"] = address\n",
    "    table[\"WaterType\"] = watertype\n",
    "    table[\"Treatment\"] = treatment\n",
    "    table[\"Sampler\"] = sampler\n",
    "    \n",
    "    # For debugging purpose\n",
    "    table[\"filepath\"] = filepath\n",
    "    \n",
    "    return table\n",
    "\n",
    "\n",
    "def extract_echantillon_code(texts, LAB=4):\n",
    "    echantillon_code = \"\"\n",
    "    \n",
    "    echantillon_keyword = \"échantillon n°\"\n",
    "    line = [t for t in texts if echantillon_keyword in t]\n",
    "    if len(line) == 0:\n",
    "        print(\"extract_echantillon(): Echantillon Unfound\")\n",
    "        return None\n",
    "    \n",
    "    if LAB == 1 or LAB == 4 or LAB == 5:\n",
    "        line = line[0]\n",
    "        echantillon_code = line[line.index(echantillon_keyword)+len(echantillon_keyword):].split()[0]\n",
    "        \n",
    "    return echantillon_code.strip()\n",
    "\n",
    "\n",
    "def extract_flocons_nb(texts, LAB=4):\n",
    "    flocons_nb = \"\"\n",
    "    \n",
    "    flocons_nb_keyword = \"Nbre de flacons: \"\n",
    "    line = [t for t in texts if flocons_nb_keyword in t]\n",
    "    if len(line) == 0:\n",
    "        print(\"extract_flocons_nb(): Flocons Nb Unfound\")\n",
    "        return None\n",
    "    \n",
    "    if LAB == 4 or LAB == 5:\n",
    "        flocons_nb = line[0].replace(flocons_nb_keyword, '')\n",
    "    \n",
    "    if flocons_nb.isdigit() and int(flocons_nb) > 0:\n",
    "        flocons_nb = int(flocons_nb)\n",
    "    \n",
    "    return flocons_nb\n",
    "\n",
    "\n",
    "# texts without treatment\n",
    "def extract_address(texts, LAB=4):\n",
    "    address = \"\"\n",
    "    \n",
    "    address_keyword = \"Point de prélèvement: \"\n",
    "    if LAB == 1:\n",
    "        address_keyword = \"Point de prélèvement\"\n",
    "    line = [t for t in texts if address_keyword in t]\n",
    "    if len(line) == 0:\n",
    "        print(\"extract_address(): Address Unfound\")\n",
    "        return None\n",
    "    next_line = texts[texts.index(line[0])+1]\n",
    "    if LAB == 1 or LAB == 4 or LAB == 5:\n",
    "        address = line[0].replace(address_keyword, '').strip()\n",
    "        if len(next_line.strip()) > 0:\n",
    "            address += ', ' + next_line\n",
    "    \n",
    "    return address.strip()\n",
    "\n",
    "\n",
    "def extract_treatment(texts, LAB=4):\n",
    "    treatment = \"\"\n",
    "    \n",
    "    treatment_keyword_dict = {1: \"\", 2: \"\", 3: \"\", 4: \"Traitement utilisé: \", 5: \"Traitement utilisé: \"}\n",
    "    \n",
    "    treatment_keyword = treatment_keyword_dict[LAB]\n",
    "    line = [t for t in texts if treatment_keyword in t]\n",
    "    if len(line) == 0:\n",
    "        print(\"extract_treatment(): Treatment Unfound\")\n",
    "        return None\n",
    "    \n",
    "    if LAB == 4 or LAB == 5:\n",
    "        treatment = line[0].replace(treatment_keyword, '')\n",
    "    \n",
    "    return treatment.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = []\n",
    "done_ABL_RWB_fps = []\n",
    "undone_ABL_RWB_fps = []\n",
    "loop_idx = 0\n",
    "\n",
    "for fp in ABL_RWB_FILES[loop_idx:]:\n",
    "    print(fp)\n",
    "    try:\n",
    "        table = extractor_ABL_RWB(fp)\n",
    "        buffer.append(table)\n",
    "        done_ABL_RWB_fps.append(fp)\n",
    "    except:\n",
    "        undone_ABL_RWB_fps.append(fp)\n",
    "    loop_idx += 1\n",
    "    \n",
    "df = pd.concat(buffer, ignore_index=True)\n",
    "df.to_pickle(\"df_ABL_RWB.pkl\")\n",
    "\n",
    "textfile = open(\"undone_ABL_RWB.txt\", \"w\")\n",
    "for fp in undone_ABL_RWB_fps:\n",
    "    textfile.write(fp + \"\\n\")\n",
    "textfile.close()\n",
    "\n",
    "textfile = open(\"done_ABL_RWB.txt\", \"w\")\n",
    "for fp in done_ABL_RWB_fps:\n",
    "    textfile.write(fp + \"\\n\")\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LAB : RUFER LAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_echantillon_dict(ocr_text):\n",
    "    echantillons_dict = {}\n",
    "    echantillon_w = \"Point de prélèvement\"\n",
    "    \n",
    "    # Check first page and retrieve all the echantillon till there is \"Remarque\"\n",
    "    echantillon_w_start_idx = [True if echantillon_w in w else False for w in ocr_text[0]].index(True)\n",
    "    echantillon_w_end_idx = [True if \"Remarque\" in w else False for w in ocr_text[0]].index(True)\n",
    "\n",
    "    # Retrieve all echantillons by splitting it with :\n",
    "    potential_echantillons = [w for w in ocr_text[0][echantillon_w_start_idx:echantillon_w_end_idx] if \":\" in w]\n",
    "    for echantillon in potential_echantillons:\n",
    "        splitted = echantillon.split(\":\")\n",
    "        code = splitted[-2].split()[-1]\n",
    "        name = splitted[-1].strip()\n",
    "        if code not in echantillons_dict.keys():\n",
    "            echantillons_dict[code] = name\n",
    "        else:\n",
    "            newcode = str(int(list(echantillons_dict.keys())[-1])+1)\n",
    "            echantillons_dict[newcode] = name\n",
    "    \n",
    "    return echantillons_dict\n",
    "\n",
    "\n",
    "# Post-process table\n",
    "# 0. Drop NaN column (with column header Unnamed) & Drop NaN row\n",
    "# 1. Merge if a column has only 1 value with its preceding column\n",
    "# 2. Merge if Unit column splitted\n",
    "# 3. Result column splitted because of < or <=\n",
    "def post_process_table(df, rufer_columns=None):\n",
    "    df = df.copy()\n",
    "    # Drop NaN columns\n",
    "    # ! the column header has to be Unnamed too !\n",
    "    for c in df.columns:\n",
    "        if (is_unnamed(c) or c == \":\") and len(df[c].dropna()) == 0:\n",
    "            df.drop(columns=[c], inplace=True)\n",
    "            \n",
    "    # Drop NaN rows\n",
    "    df.dropna(axis=0, how='all', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    \n",
    "    # 1st case: one part of the preceding column has been expanded to a new columns\n",
    "    # Typically such column has only one value in the column and all the other rows are NaN\n",
    "    for j in range(3):\n",
    "        for col_idx, c in enumerate(df.columns):\n",
    "            unique_val = df[c].dropna()\n",
    "            if len(unique_val) == 1:\n",
    "                r_idx = unique_val.index[0]\n",
    "                df = merge_two_columns(df, col_idx-1, col_idx)\n",
    "                break\n",
    "        \n",
    "    # 2nd case: Unit columns separted into two distinct columns\n",
    "    if rufer_columns:\n",
    "        unite_col = rufer_columns.index(\"Unite\")\n",
    "    else:\n",
    "        unite_col = len(df)-1\n",
    "    \n",
    "    Unite_keywords = ['C', '°C', 'ml', 'C /l', '°F', 'mg/l', 'ng/l', 'cm-1', 'UFC/100 ml']\n",
    "    if len(df.columns) > unite_col+1 and np.any(df[df.columns[unite_col+1]].astype(str).map(lambda x: x in (Unite_keywords)).values):\n",
    "    #if np.any([True if v.isin(Unite_keywords) else False for v in df[df.columns[unite_col+1]].values]):\n",
    "        df = merge_two_columns(df, unite_col, unite_col+1)\n",
    "        \n",
    "    # 3rd case : the result column has been split into two (because of < or <=)\n",
    "    last_idx = 0\n",
    "    while(last_idx < len(df.columns)-1):\n",
    "        tmp_idx = last_idx\n",
    "        for col_idx, c in enumerate(list(df.columns)[last_idx:]):\n",
    "            tmp_idx = col_idx\n",
    "            tmp_vals = df[c].dropna().unique().tolist()\n",
    "            if len(tmp_vals) > 0 and np.all([x in [\"<\", \"<=\", \">\", \">=\", \"~\"] for x in tmp_vals]):\n",
    "                df = merge_two_columns(df, last_idx+col_idx, last_idx+col_idx+1)\n",
    "                break\n",
    "        last_idx += tmp_idx\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_information_rufer(df, ocr_text, echantillon_dict):\n",
    "    df = df.copy()\n",
    "    # Remove Heure de prelevement\n",
    "    rm_idxs = list(df[df.Parametre.apply(lambda x: len(difflib.get_close_matches(str(x), [\"Heure de prélevement\"])) > 0)].index)\n",
    "    if len(rm_idxs) > 0:\n",
    "        df.drop(rm_idxs, inplace=True)\n",
    "    \n",
    "    # Extract FloconsNb\n",
    "    nb_flacons = df[df.Parametre.apply(lambda x: len(difflib.get_close_matches(str(x), [\"Nombre de flacons\", \"Nb flacons\", \"flacons\"])) > 0)][[\"Code\", \"Value\"]]\n",
    "    nb_flacons_dict = dict(zip(nb_flacons[\"Code\"], nb_flacons[\"Value\"]))\n",
    "    df[\"FloconsNb\"] = df.Code.map(lambda x: nb_flacons_dict[x] if x in nb_flacons_dict.keys() else None)\n",
    "    \n",
    "    # Extract Temperature\n",
    "    temperatures = df[df.Parametre.apply(lambda x: len(difflib.get_close_matches(str(x), [\"Temperature\"])) > 0)][[\"Code\", \"Value\"]]\n",
    "    temperatures_dict = dict(zip(temperatures[\"Code\"], temperatures[\"Value\"]))\n",
    "    df[\"Temperature\"] = df.Code.map(lambda x: temperatures_dict[x] if x in temperatures_dict.keys() else None)\n",
    "    \n",
    "    # Extract Treatment\n",
    "    treatment = df[df.Parametre.apply(lambda x: len(difflib.get_close_matches(str(x), [\"Traitement\"])) > 0)][[\"Code\", \"Value\"]]\n",
    "    treatment_dict = dict(zip(treatment[\"Code\"], treatment[\"Value\"]))\n",
    "    df[\"Treatment\"] = df.Code.map(lambda x: treatment_dict[x] if x in treatment_dict.keys() else None)\n",
    "    \n",
    "    # Extract WaterType\n",
    "    watertype = extract_watertype(ocr_text[0], LAB=1)\n",
    "    df[\"WaterType\"] = watertype\n",
    "    \n",
    "    if echantillon_dict:\n",
    "        # Extract Address\n",
    "        df[\"Address\"] = df.Code.map(lambda x: echantillon_dict[x] if x in echantillon_dict.keys() else None)\n",
    "    else:\n",
    "        df[\"Address\"] = extract_address(ocr_text[0], LAB=1)\n",
    "    \n",
    "    # Extract Sampler\n",
    "    sampler = extract_sampler(ocr_text, LAB=1)\n",
    "    df[\"Sampler\"] = sampler\n",
    "    \n",
    "    # Extract Limit\n",
    "    df[\"Limit\"] = None\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def extractor_Rufer(filepath, manual=None, typeII=False):\n",
    "    Rufer_Columns_2021 = [\"Parametre\", \"Methode\", \"Date\", \"Unite\"]\n",
    "    Rufer_Columns_TypeII = [\"Parametre\", \"Methodd\", \"Value\", \"Unite\"]\n",
    "    \n",
    "    dfs = extract_table(filepath)\n",
    "    pgNb = get_nb_pages(filepath)\n",
    "\n",
    "    #if check_image_based_pdf(filepath):\n",
    "    #    new_filepath = from_image_based_to_searchable_pdf(filepath)\n",
    "    #else:\n",
    "    #    new_filepath = filepath\n",
    "    new_filepath = filepath\n",
    "    \n",
    "    #doc = convert_from_path(new_filepath)\n",
    "    ocr_text = pytesseract_text(new_filepath)\n",
    "    \n",
    "    dfs = get_table(new_filepath)\n",
    "    \n",
    "    echantillon_dict = None\n",
    "    \n",
    "    tmp = []\n",
    "    if manual:\n",
    "        tmp = manual\n",
    "        echantillon_dict = get_echantillon_dict(ocr_text)\n",
    "    # Case I : numerous echantillons per document\n",
    "    elif pgNb > 1 and not typeII:\n",
    "        echantillon_dict = get_echantillon_dict(ocr_text)\n",
    "        \n",
    "        for df in dfs:\n",
    "            df = post_process_table(df, Rufer_Columns_2021)\n",
    "            display(df)\n",
    "            # Rename columns\n",
    "            new_column_names = Rufer_Columns_2021 + list(echantillon_dict.keys())[:len(df.columns) - len(Rufer_Columns_2021)]\n",
    "            df.columns = new_column_names + ['Value']\n",
    "            df[\"Code\"] = extract_echantillon_code(ocr_text, LAB=1)#'211946'\n",
    "            '''df.columns = new_column_names\n",
    "            present_ech = list(set(echantillon_dict.keys()).intersection(df.columns))\n",
    "            for k in present_ech:\n",
    "                df[k] = df[k].map(lambda x: (k, x))\n",
    "            df[\"Value\"] = df[present_ech].apply(tuple, axis=1)\n",
    "            df = df.drop(columns=present_ech)\n",
    "            df = df.explode(\"Value\")\n",
    "            df[\"Code\"] = df[\"Value\"].map(lambda x: x[0])\n",
    "            df[\"Value\"] = df[\"Value\"].map(lambda x: x[1])'''\n",
    "    \n",
    "            display(df)\n",
    "        \n",
    "            tmp.append(df)\n",
    "\n",
    "    # Case II : 1 echantillon per document\n",
    "    else:\n",
    "        # Assume two tables as one page\n",
    "        if pgNb == 1:\n",
    "            dfs[0].insert(1, \"Dummy\", [None for _ in range(len(dfs[0]))])\n",
    "            for df in dfs:\n",
    "                df = post_process_table(df)\n",
    "                if len(df.columns) < len(Rufer_Columns_TypeII):\n",
    "                    df.insert(len(df.columns), \"Unite\", [None for _ in range(len(df))])\n",
    "                elif len(df.columns) > len(Rufer_Columns_TypeII):\n",
    "                    df = merge_two_columns(df, len(Rufer_Columns_TypeII)-1, len(Rufer_Columns_TypeII))\n",
    "\n",
    "                if len(df.columns) == len(Rufer_Columns_TypeII):\n",
    "                    df.columns = Rufer_Columns_TypeII\n",
    "                    df[\"Code\"] = extract_echantillon_code(ocr_text, LAB=1)\n",
    "                    tmp.append(df)\n",
    "        else:\n",
    "            dfs[0].insert(1, \"Dummy\", [None for _ in range(len(dfs[0]))])\n",
    "            '''if len(dfs) > 2:\n",
    "                for i in range(2, len(dfs)):\n",
    "                    if len(dfs[i].columns) == 5:\n",
    "                        #dfs[i].drop(list(dfs[i].columns)[2], axis=1, inplace=True)\n",
    "                        display(dfs[i])'''\n",
    "            \n",
    "            for df in dfs:\n",
    "                df = post_process_table(df)\n",
    "                if len(df.columns) < len(Rufer_Columns_TypeII):\n",
    "                    df.insert(len(df.columns), \"Unite\", [None for _ in range(len(df))])\n",
    "                elif len(df.columns) > len(Rufer_Columns_TypeII):\n",
    "                    df = merge_two_columns(df, len(Rufer_Columns_TypeII)-1, len(Rufer_Columns_TypeII))\n",
    "                if len(df.columns) == len(Rufer_Columns_TypeII):\n",
    "                    df.columns = Rufer_Columns_TypeII\n",
    "                    df[\"Code\"] = extract_echantillon_code(ocr_text[0], LAB=1)\n",
    "                    tmp.append(df)\n",
    "        \n",
    "            display(df)\n",
    "\n",
    "    df = pd.concat(tmp).reset_index(drop=True)\n",
    "    \n",
    "    date = extract_date(ocr_text[0], LAB=1)\n",
    "    df[\"Date\"] = date\n",
    "    df[\"Limit\"] = None\n",
    "    \n",
    "    df = extract_information_rufer(df, ocr_text, echantillon_dict)\n",
    "    if \"Methode\" in df.columns:\n",
    "        df.drop(columns=[\"Methode\"], inplace=True)\n",
    "    \n",
    "    # For debugging purpose\n",
    "    df[\"filepath\"] = filepath\n",
    "    \n",
    "    # Date NA fill\n",
    "    df.Date = df.Date.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "    \n",
    "    return df[[\"Parametre\", \"Date\", \"Value\", \"Limit\", \"Unite\", \"Code\", \"FloconsNb\", \"Temperature\", \"Address\", \"WaterType\", \"Treatment\", \"Sampler\", \"filepath\"]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "done_Rufer_fps = []\n",
    "textfile = open(\"done_Rufer.txt\", \"r\")\n",
    "for l in textfile:\n",
    "    done_Rufer_fps.append(l.replace(\"\\n\", \"\"))\n",
    "textfile.close()\n",
    "\n",
    "undone_Rufer_fps = []\n",
    "textfile = open(\"undone_Rufer.txt\", \"r\")\n",
    "for l in textfile:\n",
    "    undone_Rufer_fps.append(l.replace(\"\\n\", \"\"))\n",
    "textfile.close()\n",
    "\n",
    "df1 = pd.read_pickle(\"df_Rufer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = []\n",
    "done_Rufer_fps = []\n",
    "undone_Rufer_fps = []\n",
    "loop_idx = 0\n",
    "\n",
    "for fp in RUFERLAB_FILES[loop_idx:]:\n",
    "    print(fp)\n",
    "    try:\n",
    "        table = extractor_Rufer(fp, manual=tmps, typeII=False)\n",
    "        buffer.append(table)\n",
    "        done_Rufer_fps.append(fp)\n",
    "        loop_idx += 1\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        undone_Rufer_fps.append(fp)\n",
    "\n",
    "df = pd.concat(buffer, ignore_index=True)\n",
    "df.to_pickle(\"df_Rufer.pkl\")\n",
    "\n",
    "textfile = open(\"undone_Rufer.txt\", \"w\")\n",
    "for fp in undone_Rufer_fps:\n",
    "    textfile.write(fp + \"\\n\")\n",
    "textfile.close()\n",
    "\n",
    "textfile = open(\"done_Rufer.txt\", \"w\")\n",
    "for fp in done_Rufer_fps:\n",
    "    textfile.write(fp + \"\\n\")\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parametre</th>\n",
       "      <th>Date</th>\n",
       "      <th>Value</th>\n",
       "      <th>Unite</th>\n",
       "      <th>Limit</th>\n",
       "      <th>Code</th>\n",
       "      <th>FloconsNb</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Address</th>\n",
       "      <th>WaterType</th>\n",
       "      <th>Treatment</th>\n",
       "      <th>Sampler</th>\n",
       "      <th>filepath</th>\n",
       "      <th>Village</th>\n",
       "      <th>Commune</th>\n",
       "      <th>Distributeur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Température mesurée in-situ</td>\n",
       "      <td>10.04.12</td>\n",
       "      <td>11.7</td>\n",
       "      <td>°C</td>\n",
       "      <td>None</td>\n",
       "      <td>1892</td>\n",
       "      <td>1</td>\n",
       "      <td>11.7</td>\n",
       "      <td>Bassecourt, eau de réseau, Espace Industriel 3...</td>\n",
       "      <td>Eau brute</td>\n",
       "      <td>UV</td>\n",
       "      <td>None</td>\n",
       "      <td>./Data/Autocontrole_EP/Bassecourt/2012/01892.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>11.04.12</td>\n",
       "      <td>0</td>\n",
       "      <td>germes/100ml</td>\n",
       "      <td>None</td>\n",
       "      <td>1892</td>\n",
       "      <td>1</td>\n",
       "      <td>11.7</td>\n",
       "      <td>Bassecourt, eau de réseau, Espace Industriel 3...</td>\n",
       "      <td>Eau brute</td>\n",
       "      <td>UV</td>\n",
       "      <td>None</td>\n",
       "      <td>./Data/Autocontrole_EP/Bassecourt/2012/01892.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Enterocoques</td>\n",
       "      <td>11.04.12</td>\n",
       "      <td>0</td>\n",
       "      <td>germes/100ml</td>\n",
       "      <td>None</td>\n",
       "      <td>1892</td>\n",
       "      <td>1</td>\n",
       "      <td>11.7</td>\n",
       "      <td>Bassecourt, eau de réseau, Espace Industriel 3...</td>\n",
       "      <td>Eau brute</td>\n",
       "      <td>UV</td>\n",
       "      <td>None</td>\n",
       "      <td>./Data/Autocontrole_EP/Bassecourt/2012/01892.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Germes aerobies</td>\n",
       "      <td>11.04.12</td>\n",
       "      <td>0</td>\n",
       "      <td>germes/ml</td>\n",
       "      <td>None</td>\n",
       "      <td>1892</td>\n",
       "      <td>1</td>\n",
       "      <td>11.7</td>\n",
       "      <td>Bassecourt, eau de réseau, Espace Industriel 3...</td>\n",
       "      <td>Eau brute</td>\n",
       "      <td>UV</td>\n",
       "      <td>None</td>\n",
       "      <td>./Data/Autocontrole_EP/Bassecourt/2012/01892.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Température mesurée in-situ</td>\n",
       "      <td>10.04.12</td>\n",
       "      <td>12.7</td>\n",
       "      <td>°C</td>\n",
       "      <td>None</td>\n",
       "      <td>1893</td>\n",
       "      <td>1</td>\n",
       "      <td>12.7</td>\n",
       "      <td>Bassecourt, eau de réseau, Rue abbé Monnin, FMB</td>\n",
       "      <td>Eau de nappe brute</td>\n",
       "      <td>UV</td>\n",
       "      <td>None</td>\n",
       "      <td>./Data/Autocontrole_EP/Bassecourt/2012/01893.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21160</th>\n",
       "      <td>Chlore libre</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.02</td>\n",
       "      <td>None</td>\n",
       "      <td>180778</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Réservoir 1500 m° Z1, Remarque</td>\n",
       "      <td>Eau</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stalder Joël</td>\n",
       "      <td>./Data/Autocontrole_EP/Boncourt/2018/20180427/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21161</th>\n",
       "      <td>Conditions météo</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>Beau</td>\n",
       "      <td>None</td>\n",
       "      <td>180778</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Réservoir 1500 m° Z1, Remarque</td>\n",
       "      <td>Eau</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stalder Joël</td>\n",
       "      <td>./Data/Autocontrole_EP/Boncourt/2018/20180427/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21162</th>\n",
       "      <td>Escherichia Coli</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>germes/100 ml</td>\n",
       "      <td>None</td>\n",
       "      <td>180778</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Réservoir 1500 m° Z1, Remarque</td>\n",
       "      <td>Eau</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stalder Joël</td>\n",
       "      <td>./Data/Autocontrole_EP/Boncourt/2018/20180427/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21163</th>\n",
       "      <td>Entérocoques</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>germes/100 ml</td>\n",
       "      <td>None</td>\n",
       "      <td>180778</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Réservoir 1500 m° Z1, Remarque</td>\n",
       "      <td>Eau</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stalder Joël</td>\n",
       "      <td>./Data/Autocontrole_EP/Boncourt/2018/20180427/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21164</th>\n",
       "      <td>Germes aérobies</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>germes/ml</td>\n",
       "      <td>None</td>\n",
       "      <td>180778</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Réservoir 1500 m° Z1, Remarque</td>\n",
       "      <td>Eau</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stalder Joël</td>\n",
       "      <td>./Data/Autocontrole_EP/Boncourt/2018/20180427/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21165 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Parametre      Date Value          Unite Limit  \\\n",
       "0      Température mesurée in-situ  10.04.12  11.7             °C  None   \n",
       "1                 Escherichia coli  11.04.12     0   germes/100ml  None   \n",
       "2                     Enterocoques  11.04.12     0   germes/100ml  None   \n",
       "3                  Germes aerobies  11.04.12     0      germes/ml  None   \n",
       "4      Température mesurée in-situ  10.04.12  12.7             °C  None   \n",
       "...                            ...       ...   ...            ...   ...   \n",
       "21160                 Chlore libre             NaN           0.02  None   \n",
       "21161             Conditions météo             NaN           Beau  None   \n",
       "21162             Escherichia Coli               0  germes/100 ml  None   \n",
       "21163                 Entérocoques             NaN  germes/100 ml  None   \n",
       "21164              Germes aérobies               0      germes/ml  None   \n",
       "\n",
       "         Code FloconsNb Temperature  \\\n",
       "0        1892         1        11.7   \n",
       "1        1892         1        11.7   \n",
       "2        1892         1        11.7   \n",
       "3        1892         1        11.7   \n",
       "4        1893         1        12.7   \n",
       "...       ...       ...         ...   \n",
       "21160  180778       NaN         NaN   \n",
       "21161  180778       NaN         NaN   \n",
       "21162  180778       NaN         NaN   \n",
       "21163  180778       NaN         NaN   \n",
       "21164  180778       NaN         NaN   \n",
       "\n",
       "                                                 Address           WaterType  \\\n",
       "0      Bassecourt, eau de réseau, Espace Industriel 3...           Eau brute   \n",
       "1      Bassecourt, eau de réseau, Espace Industriel 3...           Eau brute   \n",
       "2      Bassecourt, eau de réseau, Espace Industriel 3...           Eau brute   \n",
       "3      Bassecourt, eau de réseau, Espace Industriel 3...           Eau brute   \n",
       "4        Bassecourt, eau de réseau, Rue abbé Monnin, FMB  Eau de nappe brute   \n",
       "...                                                  ...                 ...   \n",
       "21160                     Réservoir 1500 m° Z1, Remarque                 Eau   \n",
       "21161                     Réservoir 1500 m° Z1, Remarque                 Eau   \n",
       "21162                     Réservoir 1500 m° Z1, Remarque                 Eau   \n",
       "21163                     Réservoir 1500 m° Z1, Remarque                 Eau   \n",
       "21164                     Réservoir 1500 m° Z1, Remarque                 Eau   \n",
       "\n",
       "      Treatment       Sampler  \\\n",
       "0            UV          None   \n",
       "1            UV          None   \n",
       "2            UV          None   \n",
       "3            UV          None   \n",
       "4            UV          None   \n",
       "...         ...           ...   \n",
       "21160       NaN  Stalder Joël   \n",
       "21161       NaN  Stalder Joël   \n",
       "21162       NaN  Stalder Joël   \n",
       "21163       NaN  Stalder Joël   \n",
       "21164       NaN  Stalder Joël   \n",
       "\n",
       "                                                filepath Village Commune  \\\n",
       "0       ./Data/Autocontrole_EP/Bassecourt/2012/01892.pdf     NaN     NaN   \n",
       "1       ./Data/Autocontrole_EP/Bassecourt/2012/01892.pdf     NaN     NaN   \n",
       "2       ./Data/Autocontrole_EP/Bassecourt/2012/01892.pdf     NaN     NaN   \n",
       "3       ./Data/Autocontrole_EP/Bassecourt/2012/01892.pdf     NaN     NaN   \n",
       "4       ./Data/Autocontrole_EP/Bassecourt/2012/01893.pdf     NaN     NaN   \n",
       "...                                                  ...     ...     ...   \n",
       "21160  ./Data/Autocontrole_EP/Boncourt/2018/20180427/...     NaN     NaN   \n",
       "21161  ./Data/Autocontrole_EP/Boncourt/2018/20180427/...     NaN     NaN   \n",
       "21162  ./Data/Autocontrole_EP/Boncourt/2018/20180427/...     NaN     NaN   \n",
       "21163  ./Data/Autocontrole_EP/Boncourt/2018/20180427/...     NaN     NaN   \n",
       "21164  ./Data/Autocontrole_EP/Boncourt/2018/20180427/...     NaN     NaN   \n",
       "\n",
       "      Distributeur  \n",
       "0              NaN  \n",
       "1              NaN  \n",
       "2              NaN  \n",
       "3              NaN  \n",
       "4              NaN  \n",
       "...            ...  \n",
       "21160          NaN  \n",
       "21161          NaN  \n",
       "21162          NaN  \n",
       "21163          NaN  \n",
       "21164          NaN  \n",
       "\n",
       "[21165 rows x 16 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rufer = pd.read_pickle(\"df_Rufer.pkl\")\n",
    "df_abl = pd.read_pickle(\"df_ABL_RWB.pkl\")\n",
    "\n",
    "df = pd.concat([df_abl, df_rufer], ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Village, Commune, Distributeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "listing = pd.read_excel(\"Data/List_commune-distributeurs_EP.xlsx\")\n",
    "listing.dropna(how='all', axis=1, inplace=True)\n",
    "listing.dropna(how='all', axis=0, inplace=True)\n",
    "listing.fillna(method=\"ffill\", inplace=True)\n",
    "listing[[\"Commune\", \"Localite\"]] = listing[[\"Commune\", \"Localite\"]].applymap(lambda x: remove_accents(x.replace(\" \", \"_\").replace(string.punctuation, \"\")).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").rstrip())\n",
    "\n",
    "\n",
    "def get_commune(path, listing):\n",
    "    dir_name = path.split(\"/\")[3]\n",
    "    commune = dir_name\n",
    "    village = None\n",
    "    distributeur = None\n",
    "    \n",
    "    if dir_name in listing.Commune.values:\n",
    "        locs = listing[listing.Commune == dir_name].Localite.values\n",
    "        if dir_name in locs:\n",
    "            village = dir_name\n",
    "        elif len(locs) == 1:\n",
    "            village = locs[0]\n",
    "        else:\n",
    "            pass\n",
    "    elif dir_name in listing.Localite.values:\n",
    "        village = dir_name\n",
    "        commune = list(listing[listing.Localite == dir_name].Commune.values)[0]\n",
    "    \n",
    "    distributeur = list(listing[listing.Commune == commune].Distributeur.values)[0] if len(listing[listing.Commune == commune]) > 0 else commune\n",
    "    \n",
    "    return village, commune, distributeur\n",
    "\n",
    "\n",
    "df[\"Village\"] = df.filepath.map(lambda x: get_commune(x, listing))\n",
    "df[\"Commune\"] = df[\"Village\"].map(lambda x: x[1])\n",
    "df[\"Distributeur\"] = df[\"Village\"].map(lambda x: x[2])\n",
    "df[\"Village\"] = df[\"Village\"].map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Address = df.Address.map(lambda x: x.split(\"Remarque\")[0].strip() if x else None)\n",
    "code_Address = df.loc[df.Address[df[\"Address\"].str.contains(\":\").replace({None: False})].index]\n",
    "for i, row in code_Address.iterrows():\n",
    "    if not row.Code:\n",
    "        df.at[i, \"Code\"] = row.Address.split(\":\")[0].strip()\n",
    "df.Address[df[\"Address\"].str.contains(\":\").replace({None: False})] = df.Address[df[\"Address\"].str.contains(\":\").replace({None: False})].map(lambda x: x.split(\":\")[1].strip())\n",
    "df.Address = df[\"Address\"].map(lambda x: x.split(\", (identification\")[0] if x else x).map(lambda x: x.split(\", identification\")[0] if x else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Eau brute', 'Eau traitee', None, 'Reseau', 'Reservoir'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def water_type_from_address(address):\n",
    "    water_type = None\n",
    "    if \"Source\" in address or \"Fontaine\" in address:\n",
    "        water_type = \"Eau brute\"\n",
    "    \n",
    "    elif \"traitement\" in address:\n",
    "        water_type = \"Eau traitee\"\n",
    "    \n",
    "    elif \"Réservoir\" in address or \"Réservair\" in address:\n",
    "        water_type = \"Reservoir\"\n",
    "    \n",
    "    elif \"Réseau\" in address or \"réseau\" in address or \"Réeau\" in address:\n",
    "        water_type = \"Reseau\"\n",
    "        \n",
    "    return water_type\n",
    "\n",
    "df[\"WaterType\"] = df[\"WaterType\"].replace({'Eau': None, \"Eeu\": None, \"eau\": None, 'Eau de source brute': 'Eau brute', 'Eau de nappe brute': 'Eau brute', 'Eau de nappe filtrée': 'Eau brute', 'Réseau de Porrentruy': \"Reseau\", 'Eau de réseau Porrentruy': \"Reseau\", \"Eau de boisson\": \"Reseau\", \"Eau potable\": \"Reseau\", \"Eau de source\": \"Eau brute\", \"Eau traitée\": \"Eau traitee\"})\n",
    "df[\"WaterType\"] = df[\"WaterType\"].replace({'eau de réseau': 'Reseau', 'Eau de source désinfectée': \"Eau traitee\", 'eau SEHA': 'Reseau', 'Eau SEV': 'Reseau', 'Eau SIVAMO': 'Reseau', 'Eau 2908 Grandfontaine': None})\n",
    "for i, row in df.iterrows():\n",
    "    if row.Address and not row.WaterType:\n",
    "        df.at[i, \"WaterType\"] = water_type_from_address(row.Address)\n",
    "    \n",
    "    if row.WaterType and \"filtrée\" in str(row.WaterType) or \"désinfectée\" in str(row.WaterType) or \"désinfetée\" in str(row.WaterType):\n",
    "        df.at[i, \"WaterType\"] = \"Eau traitee\"\n",
    "    elif row.WaterType and \"nappe\" in str(row.WaterType) or \"brute\" in str(row.WaterType):\n",
    "        df.at[i, \"WaterType\"] = \"Eau brute\"\n",
    "    elif row.WaterType and (\"réseau\" in str(row.WaterType) or \"boisson\" in str(row.WaterType)):\n",
    "        df.at[i, \"WaterType\"] = \"Reseau\"\n",
    "        \n",
    "df[\"WaterType\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_input(df, col, false_kw, corrected_kw):\n",
    "    idxs = df[col][df[col] == false_kw].index\n",
    "    for i in idxs:\n",
    "        df.at[i, col] = corrected_kw\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Parameter_corr = pd.read_excel(\"parameter_corrected.xlsx\")\n",
    "df_Parameter_corr.drop(columns=\"ID\", inplace=True)\n",
    "df_Parameter_corr = df_Parameter_corr.drop(df_Parameter_corr[df_Parameter_corr.Remarque == \"à supprimer\"].index).drop(columns=\"Remarque\").reset_index(drop=True)\n",
    "df_Parameter_corr = df_Parameter_corr.assign(Unit=df_Parameter_corr[\"Correct Unit\"].fillna(df_Parameter_corr[\"Unit\"]))\n",
    "df_Parameter_corr = df_Parameter_corr.drop(columns=\"Correct Unit\")\n",
    "df_Parameter_corr.Nom = df_Parameter_corr.Nom.map(lambda x: remove_accents(x))\n",
    "df_Parameter_corr.drop_duplicates(inplace=True)\n",
    "df_Parameter_corr.at[df_Parameter_corr[df_Parameter_corr[\"Nom\"] == \"Conductivite (25 C)\"].index, \"Nom\"] = \"Conductivite (25°C)\"\n",
    "df_Parameter_corr.at[df_Parameter_corr[df_Parameter_corr[\"Nom\"] == \"Conductivite in situ 20C\"].index, \"Nom\"] = \"Conductivite in situ 20°C\"\n",
    "for i, row in df_Parameter_corr.iterrows():\n",
    "    if row.Nom.startswith(\"-\"):\n",
    "        df_Parameter_corr.at[i, \"Nom\"] = row.Nom[1:].strip()\n",
    "df_Parameter_corr.drop_duplicates(inplace=True)\n",
    "df_Parameter_corr.drop([26, 27, 96, 97, 98, 99, 100, 101], inplace=True)\n",
    "df_Parameter_corr.reset_index(drop=True, inplace=True)\n",
    "df_Parameter_corr.drop([26, 40, 144], inplace=True)\n",
    "df_Parameter_corr.reset_index(drop=True, inplace=True)\n",
    "df_Parameter_corr.to_pickle(\"df_Parameter_corr.pkl\")\n",
    "for i in range(0, len(df_Parameter_corr), 50):\n",
    "    display(df_Parameter_corr.loc[i:i+50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df.Parametre.map(lambda x: difflib.get_close_matches(str(x), list(df_Parameter_corr.Nom.unique())))\n",
    "\n",
    "Param_to_clean = df.loc[tmp[tmp.str.len()==0].index]\n",
    "Param_to_clean.Value = Param_to_clean.Value.replace({np.nan: None, None: None, \"NaN\": None, \"n/a\": None})\n",
    "Param_to_clean = Param_to_clean[Param_to_clean.Value.notna()]\n",
    "Param_to_clean = Param_to_clean[Param_to_clean.Value != 999]\n",
    "\n",
    "to_drop_idx = []\n",
    "for i, row in Param_to_clean.iterrows():\n",
    "    new_Param_name = row.Parametre\n",
    "    if row.Parametre == 'Carbone organique dissous' or row.Parametre == 'Carbone organique dissous (DOC)':\n",
    "        df.at[i, \"Parametre\"] = \"DOC\"\n",
    "    elif row.Parametre.startswith(\"Escherichia\"):\n",
    "        df.at[i, \"Parametre\"] = \"Escherichia\"\n",
    "    elif len(row.Parametre.split())>1 and (row.Parametre.split()[1].startswith(\"NF\") or row.Parametre.split()[1].startswith(\"EPA\") or row.Parametre.split()[1].startswith(\"7.2-MOD\")):\n",
    "        df.at[i, \"Parametre\"] = row.Parametre.split()[0]\n",
    "    \n",
    "    new_name = difflib.get_close_matches(str(new_Param_name), list(df_Parameter_corr.Nom.unique()))\n",
    "    if len(new_name) == 0:\n",
    "        to_drop_idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Parametre = df.Parametre.map(lambda x: difflib.get_close_matches(str(x), list(df_Parameter_corr.Nom.unique())))\n",
    "df.drop(tmp[tmp.str.len()==0].index, inplace=True)\n",
    "df.Parametre = df.Parametre.map(lambda x: x[0])\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Limit = df.Parametre.map(lambda x: dict(zip(df_Parameter_corr.Nom, df_Parameter_corr.Limit))[x])\n",
    "df.Unite = df.Parametre.map(lambda x: dict(zip(df_Parameter_corr.Nom, df_Parameter_corr.Unit))[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = correct_input(df, \"Date\", 'Mercredi 78.08.2019', 'Mercredi 07.08.2019')\n",
    "df = correct_input(df, \"Date\", '00:00:00', \"01.01.99\")\n",
    "df = correct_input(df, \"Date\", 'RP iPiPiPiPSaimPedlii P3i1.P0i8.P2i01P9 iPirPeiPirmPipPirPirPeirmirir', 'Vendredi 30.08.2019')\n",
    "df.Date = df.Date.replace({\"\": None})\n",
    "df.Date.fillna(\"01.01.99\", inplace=True)\n",
    "df.Date = df.Date.map(lambda x: \"01.01.99\" if type(x)!= str else x)\n",
    "df.Date = df.Date.map(lambda x: str(x).split()[-1]).map(lambda x: datetime.strptime(x, \"%d.%m.%y\") if x != \"nan\" and len(x) ==8 else x).map(lambda x: datetime.strptime(x, \"%d.%m.%Y\") if x != \"nan\" and type(x)==str else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in df.iterrows():\n",
    "    if str(row.Value).startswith(\"<\") or str(row.Value).startswith(\">\"):\n",
    "        newval = row.Value.split()[-1]\n",
    "        newval = newval.replace(\"<\", \"\").strip()\n",
    "        newval = newval.replace(\">\", \"\").strip()\n",
    "        df.at[i, \"Value\"] = newval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Value = df.Value.map(lambda x: x.split()[-1] if x and type(x)==str else x)\n",
    "df.Value = df.Value.map(lambda x: float(x) if type(x) == str and x.isnumeric() else x)\n",
    "df.Value = df.Value.map(lambda x: 999 if type(x) == str else x)\n",
    "df.Value = df.Value.fillna(999) #.Value.map(lambda x: 999 if type(x) == str else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Limit = df.Limit.fillna(999) # fill with impossible value to fit the dtype int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 11.7 ,  12.7 , 999.  ,  14.  ,  11.  ,  12.  ,  15.  ,  25.  ,\n",
       "        17.  ,  21.  ,   7.  ,  17.1 ,  18.3 ,   9.6 ,   8.7 ,  12.3 ,\n",
       "         9.3 ,   9.4 ,  12.5 ,  14.5 ,  13.5 ,  16.5 ,   8.5 ,  16.  ,\n",
       "        18.  ,  18.5 ,  19.  ,  11.5 ,  10.  ,   9.5 ,  10.5 ,   9.  ,\n",
       "        15.5 ,  10.2 ,  13.  ,  17.5 ,  15.2 ,  20.  ,   8.2 ,  15.8 ,\n",
       "        15.3 ,   9.2 ,  15.7 ,  13.3 ,  13.2 ,   9.9 ,  12.2 ,  15.1 ,\n",
       "        20.5 ,  16.4 ,  13.4 ,   7.2 ,  13.9 ,  12.1 ,   9.7 ,  16.2 ,\n",
       "        15.4 ,  14.1 ,   7.7 ,   8.  ,   6.  ,   5.3 ,  12.4 ,   6.8 ,\n",
       "        18.8 ,  10.6 ,   4.3 ,  14.8 ,  18.6 ,  20.4 ,  17.4 ,  19.8 ,\n",
       "        13.1 ,  10.1 ,  13.6 ,  12.8 ,  18.2 ,  14.6 ,   8.8 ,   7.1 ,\n",
       "        11.4 ,  11.1 ,  17.9 ,   7.4 ,   8.3 ,  17.2 ,  13.8 ,  12.6 ,\n",
       "         9.8 ,  10.9 ,  10.7 ,  17.7 ,  10.8 ,   6.7 ,   5.  ,   6.5 ,\n",
       "         4.  ,   7.5 ,  19.5 ,   8.1 ,   7.9 ,   7.25,   7.3 ,   7.8 ,\n",
       "        10.3 ,   9.1 ,  15.9 ,  11.6 ,  18.9 ,  21.5 ,  20.2 ,  22.6 ,\n",
       "        21.9 ,  13.7 ,  11.3 ,  14.7 ,  16.7 ,  22.1 ,  18.7 ,  11.8 ,\n",
       "        19.9 ,  12.9 ,  20.1 ,  16.8 ,  18.4 ,  16.9 ,  15.6 ,   8.9 ,\n",
       "        11.2 ,  15.03,  19.2 ,  19.6 ,  17.8 ,  19.4 ,  16.1 ,  16.3 ,\n",
       "        18.1 ,  19.1 ,  20.8 ,   3.5 ,   7.6 ,   8.4 ,  21.3 ,  21.7 ,\n",
       "        27.9 ,   4.5 ,   5.5 ,   3.  ,  22.5 ,   1.  ,  20.6 ,   8.6 ,\n",
       "        16.6 ,  14.4 ,  10.4 ,  22.4 ,  21.1 ,  22.9 ,   0.  ,  22.  ,\n",
       "         2.  ,   4.9 ,   5.8 ])"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Temperature = df.Temperature.replace({'n/a': None, '10°C': None, np.nan: None, 'N/A': None, 'n/a nan': None, 'n/a °C': None, '&': None, '°c': None, '/ °C': None, 'Eau de javelE  au 10': 10, '-3°C': 0, '9?': 9, '°C': None})\n",
    "df.Temperature = df.Temperature.replace(\"n/a\", 999)\n",
    "df.Temperature = df.Temperature.map(lambda x: x.split()[0] if x and type(x)!=float and type(x)!=int else x)#.map(lambda x: x/10.0 if x and int(x) > 50 else x)\n",
    "df.Temperature = df.Temperature.map(lambda x: float(str(x).replace(',', '.')) if x else x)\n",
    "df.Temperature = df.Temperature.map(lambda x: x/10.0 if x and x>40 and x!= 999 else x)\n",
    "df.Temperature = df.Temperature.astype(float)\n",
    "df.Temperature = df.Temperature.replace(\"n/a\", 999).fillna(999).astype(float)\n",
    "df.Temperature.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Code.fillna(999, inplace=True)\n",
    "df.Code = df.Code.map(lambda x : x.replace('?', '' ).replace('U', '') if x and type(x)==str else x)\n",
    "df.Code = df.Code.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.FloconsNb = df.FloconsNb.replace({'1 1 1': None, np.nan: None, '1 1': None, '1 nan': None, '1 1 2 1': None, ')': None, '_': None, '1.04 201': 1, 'l': 1})\n",
    "df.FloconsNb.fillna(999, inplace=True)\n",
    "df.FloconsNb = df.FloconsNb.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['UV', 'dioxyde de chlore + UF', None, 'dioxyde de chlore',\n",
       "       'ozone, eau de javel', 'UF', 'ozone', 'UV, dioxyde de chlore',\n",
       "       'UV, eau de javel', 'eau de javel', 'dioxyde de chlore, eau de',\n",
       "       'UV, dioxyde de chlore,UF', 'eau de javel, UF',\n",
       "       'ultra-filtration+eau de javel', 'ultra-filtrationteau de javel',\n",
       "       'UV, ozone', 'eau de javel, UV', 'ozone, dioxyde de chlore',\n",
       "       'charbon actif', 'UF, UV', 'dioxyde de Chlore, UF',\n",
       "       'dioxyde de chlore, chlore', 'eau de secours Porrentruy', 'None',\n",
       "       'UV et UF', 'brute', 'Javel + UF', 'Chlore/UF', 'onore/ UV UV',\n",
       "       'cores', 'Javel/UF', 'Chlore et UF', '+ chlore', 'Chlore + UV',\n",
       "       '15.2 UV', '13.1 UV', '18.5 UV', '20.4 UV', '10.0 n/a',\n",
       "       'Ozone /javel', 'Ozone /Javel', 'Brute', 'Chlore', 'Ozone / Javel',\n",
       "       'Ozone et javel', 'UV/UF', 'Chlore /UF', 'uv', 'nia', '8 UV',\n",
       "       'UF/UV', 'UV / UF', 'Ozone', 'Ozone / Chlore', 'Javel',\n",
       "       'Ozone/eau de javel', 'n/a nan', 'Eau brute', '18.5 n/a',\n",
       "       'Chlore + UF', '10 de javel', 'Eau de javelE  au 10', 'Ozone/UF',\n",
       "       'javel', '13.0 Brute', '0.03 mg/L', 'Jave/ lUF', 'Javel/UV',\n",
       "       '10.9 UV', 'Ozone /Uavel', 'Ozone/Javel', '+ Chlore',\n",
       "       'Chlore total', 'libre QO', 'Chlore, ozone', 'Chlore / UF',\n",
       "       'Chlore/UV/UF', 'UV/Javel', 'UF/Chlore', nan, 'n/a', 'n/a UF',\n",
       "       'n/a UV/UF', 'Chlore/UV', 'UV et javel', '7 UV', '9 UF', '10 UV',\n",
       "       'UV et chlore', '10.3 n/a', '13.3 n/a', 'UV/Chlore', '14.3 Ozone'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Treatment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Treatment = df.Treatment.replace({np.nan: None, '8.0 UV': \"UV\", 'n/a': None, '16.0 16.0': None, '16.5 UV' : 'UV', '10 Brute': 'Brute', '9.3 Ozone/UF': 'Ozone/UF', 'Jave/ lUV': 'Javel/UV', \"15 javel javell 14 brute 12.5 UF 16\": 'Javel', 'Chlore Chlore total libre 0.00 0.04 mg/lmg/l': 'Chlore', '|': None, 'nuageux': None, 'n/a UF': 'UF'})\n",
    "df.Treatment.fillna(\"None\", inplace=True)\n",
    "df.Treatment = df.Treatment.map(lambda x: re.sub(r'[0-9]', '', x) if type(x)==str else x)\n",
    "df.Treatment = df.Treatment.map(lambda x: x.replace('n/a', '') if type(x)==str else x)\n",
    "df.Treatment = df.Treatment.map(lambda x: x.replace('+', '/') if type(x)==str else x)\n",
    "df.Treatment = df.Treatment.map(lambda x: x.replace('et', '/') if type(x)==str else x)\n",
    "df.Treatment = df.Treatment.map(lambda x: x.replace('.', '') if type(x)==str else x)\n",
    "df.Treatment = df.Treatment.map(lambda x: x.replace(' / ', '/').replace('/ ', '/').replace(' /', '/')).map(lambda x: x.replace('.', ''))\n",
    "df.Treatment = df.Treatment.replace({'. mg/L': 'None', 'uv': 'UV', 'Eau brute': 'None', '': 'None'})\n",
    "df.Treatment = df.Treatment.map(lambda x: x.strip())\n",
    "\n",
    "df.Treatment.fillna(\"None\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Sampler = df.Sampler.map(lambda x: remove_accents(x) if x else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([None, 'Jacques Heyer', 'Yves Salomon', 'Pierre Stieger',\n",
       "       'Allemann J.-P.', 'Luginbuhl Didier', 'Pierre-Andre Guerdat',\n",
       "       'Michel Chevre', 'Michel Dupre', 'Chapuis Serge', 'Serge Gschwind',\n",
       "       'Raymond Voillat', 'Joel Stalder', 'Jean-Pierre Pataoner',\n",
       "       'Francis Droz', 'Michel Wermeille', 'Fernando Ribeiro',\n",
       "       'Florent Schori', 'Andre Chappuis', 'Valli Ugo', 'Goudron Vincent',\n",
       "       'Falbriard Gabriel', 'F. G.', 'Jean-Luc Domine', 'Herve Gerster',\n",
       "       'Gregory Jeannerat', 'Marchand Jacques', 'Allemann Blaise',\n",
       "       'Fluck K.', 'Hauser Stephane', 'Panier Didier', 'Pierre Petignat',\n",
       "       'Didier Panier', 'Christian Schneider', 'Francois Hernandez',\n",
       "       'Corbat, C.', 'Corbat C.', 'Andre Vauclair', 'Ernest Monin',\n",
       "       'Damien Belet', 'Dominique Queloz', 'Alain Baume',\n",
       "       'Jean-Pierre Coulon', 'Gerard Coulon', 'Clerc Paul',\n",
       "       'Mathieu Grossenbacher', 'Roth M.', 'Paul Crelin', 'Paul Crelier',\n",
       "       'Serge Chapuis', 'Johann Stalder', 'Chapuis S.', 'Ernest Muller',\n",
       "       'Marc Duchene', 'Pierre Voisard', 'Patrick Sudan',\n",
       "       'Richard Chevre', 'Jose Petermann', 'Andre Joray', 'Stephane Klay',\n",
       "       'Fulgido Fabio', '', 'Gregory M.', 'S. Rufer', 'M.Rich',\n",
       "       'Marc Grossenbacher', 'Rich M', 'Chagouis Serge',\n",
       "       'Seuret/Valerie Julien', 'Rich M.', 'Grosssenbacher M',\n",
       "       'S.Rufer/M.Rich', 'C.Corbat', 'Fulgido F.', 'Fuigido Fabio',\n",
       "       'Rebetez Daniel', 'Vieux Christophe', 'Chapuis Sege', 'Muller E.',\n",
       "       'Muller, E.', 'Schaller 0.', 'Olivier M.', 'Fuetterer M.',\n",
       "       'Fuetterer Benoit', 'Yves M.', 'Joray M.', 'Grossenbacher Mathieu',\n",
       "       'S.Kklay', 'du Norn', 'Jeannerat Gregoy', 'Fulganc Fabio',\n",
       "       'Rufer B.Stettler/S', 'MRich', 'Fabio Fulgano', 'Bourquard. J.',\n",
       "       'Quiquerez Serge', 'Rich Martial', 'S.Rufer/R.Marchand',\n",
       "       'Comment Richard', 'F.Fuigido', 'Petermann M.', 'D. Monsieur',\n",
       "       'Queloz M.', 'S.Rufer/D.Queloz', 'Chapouis Andre'], dtype=object)"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Sampler.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Sampler = df.Sampler.replace({'Belet Damien': 'Damien Belet', \n",
    "                                 'Belet D.': 'Damien Belet',\n",
    "                                 'Chappuis Andre': 'Andre Chappuis',\n",
    "                                 'S.Chapuis': 'Andre Chappuis',\n",
    "                                 'Chapuis M.': 'Andre Chappuis',\n",
    "                                 'M. Chevre' : 'Michel Chevre',\n",
    "                                '/ Chevre' : 'Michel Chevre',\n",
    "                                'MChevre' : 'Michel Chevre',\n",
    "                                 'Chevre M.': 'Michel Chevre', \n",
    "                                'Chevre Richard' : \"Richard Chevre\",\n",
    "                                 'Chevre R.': \"Richard Chevre\",\n",
    "                                 'Coulon JP': 'Jean-Pierre Coulon', \n",
    "                                 'Coulon Jean-Pierre': 'Jean-Pierre Coulon',\n",
    "                                 'Pierre Jean': 'Jean-Pierre Coulon', \n",
    "                                 'Jean-Pierre Monsieur': 'Jean-Pierre Coulon', \n",
    "                                 'G.Coulon': 'Jean-Pierre Coulon',\n",
    "                                 'Coulon Jean-Piere': 'Jean-Pierre Coulon', \n",
    "                                 'Couion J.-P.': 'Jean-Pierre Coulon', \n",
    "                                 'Coulon J.-P.': 'Jean-Pierre Coulon',\n",
    "                                 'Domine J.-L.': 'Jean-Luc Domine', \n",
    "                                 'J-Luc Domine': 'Jean-Luc Domine', \n",
    "                                 'Domine Jean-Luc': 'Jean-Luc Domine', \n",
    "                                 'DOMINE Jean-Luc': 'Jean-Luc Domine',\n",
    "                                 'Jean-Luc Dornine': 'Jean-Luc Domine', \n",
    "                                 'Jean-Luc Doraine': 'Jean-Luc Domine',\n",
    "                                 'Dornine M.': 'Jean-Luc Domine',\n",
    "                                 'Droz F.': 'Francis Droz',\n",
    "                                 'Droz Francis': 'Francis Droz',\n",
    "                                 'F.Droz/F.Schori': 'Francis Droz',\n",
    "                                 'F.Schori F.Droz,': 'Francis Droz',\n",
    "                                 'et F.Drez': 'Francis Droz',\n",
    "                                 'F.Schori/F.Droz': 'Francis Droz',\n",
    "                                 'Francis/Schori Droz': 'Francis Droz',\n",
    "                                 'Fulgano Febio': 'Fabio Fulgano',\n",
    "                                 'Fulgano Febio': \"Fabio Fulgano\",\n",
    "                                 'M.Grossenbacher' : 'Marc Grossenbacher',\n",
    "                                'Grossenbacher Marc' : 'Marc Grossenbacher',\n",
    "                                'Grossenbacher M.' : 'Marc Grossenbacher',\n",
    "                                'Grossenbacher M' : 'Marc Grossenbacher',\n",
    "                                'Pierre Guerdat' : 'Pierre-Andre Guerdat',\n",
    "                                 'J. Heyer': 'Jacques Heyer', \n",
    "                                 'Jacquet Heyer': 'Jacques Heyer',\n",
    "                                 'J Heyer' : 'Jacques Heyer',\n",
    "                                 'FHernandez': 'Francois Hernandez',\n",
    "                                 'François Hernandez': 'Francois Hernandez',\n",
    "                                 'F.Hernandez': 'Francois Hernandez',\n",
    "                                 'Hernandez F.': 'Francois Hernandez',\n",
    "                                 'F. Hernandez': 'Francois Hernandez',\n",
    "                                 'Hernandez Francois': \"Francois Hernandez\",\n",
    "                                 'Jeannerat Gregory': 'Gregory Jeannerat', \n",
    "                                 'G.Jeannerat': 'Gregory Jeannerat', \n",
    "                                 'Jeannerat G.': 'Gregory Jeannerat',\n",
    "                                 'Joray A.': 'Andre Joray',\n",
    "                                 'AA.Joray': 'Andre Joray',\n",
    "                                 'Joray A,': 'Andre Joray',\n",
    "                                 'A.Joray' : 'Andre Joray',\n",
    "                                 'Joray Andre' : 'Andre Joray',\n",
    "                                 'Marchand J.': 'Marchand Jacques',\n",
    "                                 'Klay Stephane': 'Stephane Klay',\n",
    "                                 'S.K!ay': 'Stephane Klay', \n",
    "                                 '/M.Cortat M.Klay': 'Stephane Klay', \n",
    "                                 'S.Klay': 'Stephane Klay',\n",
    "                                 'S.Klay/R.Cortat': 'Stephane Klay',\n",
    "                                 'Emest Monin': 'Ernest Monin',\n",
    "                                 'Belet Monin/': 'Ernest Monin',\n",
    "                                 '/ Monin': 'Ernest Monin', \n",
    "                                 '- Monin': 'Ernest Monin',\n",
    "                                 'E.Muller': 'Ernest Muller',\n",
    "                                 'Muller Ernest': 'Ernest Muller',\n",
    "                                 'E.Muller' : 'Ernest Muller',\n",
    "                                 'M.Muller' : 'Ernest Muller',\n",
    "                                 'Muller M.' : 'Ernest Muller',\n",
    "                                 'EMulter/S.Rufer': \"Ernest Muller\",\n",
    "                                 'Muller/ Ernest' : 'Ernest Muller',\n",
    "                                 'J-Pierre Pataoner' : 'Jean-Pierre Pataoner',\n",
    "                                 'M.Petermann': 'Jose Petermann', \n",
    "                                 'Petermann Josee': 'Jose Petermann', \n",
    "                                 'J.Petermann': 'Jose Petermann',\n",
    "                                 'Petermann J.': 'Jose Petermann',\n",
    "                                 'Petermann Jose': 'Jose Petermann',\n",
    "                                 'S.Rufer': \"S. Rufer\", \n",
    "                                 '/ S.Rufer': \"S. Rufer\",\n",
    "                                 'S.Rufert': \"S. Rufer\",\n",
    "                                 'Rufer S.': \"S. Rufer\",\n",
    "                                 'S.Rufer. R.Marchand,': \"S. Rufer\",\n",
    "                                 '/S.Rufer G.Meyer': \"S. Rufer\",\n",
    "                                 '/S.Rufer B.Stettler': \"S. Rufer\",\n",
    "                                 '/B.Stettler S.Rufer': \"S. Rufer\",\n",
    "                                 '/ S.Rufer/R.Marchand': \"S. Rufer\",\n",
    "                                 'S.Rufer/R.Marchand' : \"S. Rufer\",\n",
    "                                 'Baumlin/S.Rufer M.': \"S. Rufer\",\n",
    "                                 'Schori Messieurs': \"Florent Schori\",\n",
    "                                 'Schori Florent': \"Florent Schori\",\n",
    "                                 'F.Schori': \"Florent Schori\",\n",
    "                                 'Schori F.': \"Florent Schori\",\n",
    "                                'Salomon Yves' : 'Yves Salomon', \n",
    "                                 'Stelder Joel': 'Joel Stalder',\n",
    "                                 'Siatder J.': 'Joel Stalder',\n",
    "                                 'Stalder Joel': 'Joel Stalder', \n",
    "                                 'Stalder J.': 'Joel Stalder', \n",
    "                                 'J.Stalder': 'Joel Stalder', \n",
    "                                 'M.Stalder': 'Joel Stalder',\n",
    "                                 'Joel M.': 'Joel Stalder',\n",
    "                                 'J.Stalcier': 'Joel Stalder',\n",
    "                                 '/ Wermeille': 'Michel Wermeille',\n",
    "                                'Wermeille M.': 'Michel Wermeille',\n",
    "                                 'n/a': None, \n",
    "                                 'du om': None,\n",
    "                                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Temperature mesuree in-situ' 'Escherichia coli' 'Enterocoques'\n",
      " 'Germes aerobies' 'Chlore libre in-situ' 'Chlore total in-situ' 'pH'\n",
      " 'Conductivite (25°C)' 'Turbidite' 'Absorption UV 254' 'DOC' 'Oxydabilite'\n",
      " 'Alcalinite' 'Durete' 'Potassium' 'Sodium' 'Calcium' 'Magnesium'\n",
      " 'Ammonium' 'Nitrite' 'Phosphate' 'Chlorure' 'Sulfate' 'Nitrate'\n",
      " 'Durete totale' 'Temperature' '1,1-Dichloroethylene'\n",
      " 'Trans-1,2-Dichloroethylene' 'Cis-1,2-Dichloroethylene'\n",
      " 'Chlorure de methylene' 'Chloroforme' '1,1,1 Trichlorethane'\n",
      " 'Tetrachlorure de carbone' 'Trichlorethylene' 'Dichlorobromomethane'\n",
      " 'Dibromochloromethane' '1,1,2 Trichlorethane' 'Perchlorethylene'\n",
      " 'Bromoforme' 'Pesticides communes' 'Alachlor' 'Atrazine' 'Bromoxynil'\n",
      " 'Chlorbromuron' 'Chlortoluron' 'Chlorpyriphos' 'Cyanazine'\n",
      " 'Desethylatrazine' 'Deisopropylatrazine' 'Dinoseb' 'Diuron' 'alpha-HCH'\n",
      " 'beta-HCH' 'alpha-endosulfan' 'beta-endosulfan' 'Endosulfan  sulfate'\n",
      " 'Fenpropimorphe' 'Hexazinon' 'Ioxynil' 'Isoproturon' 'Lindane' 'Linuron'\n",
      " 'Malathion' 'Metalaxyl' 'Metamitron' 'Metolachlor' 'Pendimethaline'\n",
      " 'Pirimicarbe' 'Prometryne' 'Propazine' 'Sebuthylazine' 'Simazine'\n",
      " 'sulcotrione' 'Terbutylazine' 'Terbutryne' 'Tetrachlorvinphos'\n",
      " 'Trifluraline' 'Chlore' 'Benzotriazol' 'Mecoprop (MCPP)' 'Carbamazepine'\n",
      " 'Sulfamethoxazole' 'Diclofenac' 'Micropolluants-1' 'Carbofurane'\n",
      " '1,1- Dichloroethene' 'trans-1,2-Dichloroethene' '1,1-Dichloroethane'\n",
      " '2,2-Dichloropropane' 'cis-1,2-Dichloroethene' 'Bromochloromethane'\n",
      " '1,1-Dichloropropene' '1,2-Dichloroethane' 'Benzene'\n",
      " '1,2-Dichloropropane' 'Dibromomethane' 'cis-1,3-Dichloropropene'\n",
      " 'Toluene' 'trans-1,3-Dichloropropene' '1,1,2-Trichloroethane'\n",
      " '1,3-Dichloropropane' '1,2-dibromoethane' 'Chlorobenzene'\n",
      " '1,1,1,2-Tetrachloroethane' 'Ethylbenzene' 'm- + p-Xylenes' 'o-Xylene'\n",
      " 'Styrene' 'Isopropylbenzene' '1,1,2,2-Tetrachloroethane'\n",
      " '1,2,3-Trichloropropane' 'n-Propylbenzene' 'Bromobenzene'\n",
      " '1,3,5-Trimethylbenzene' '2-Chlorotoluene' '4-Chlorotoluene'\n",
      " 'tert-Butylbenzene' '1,2,4-Trimethylbenzene' 'sec-Butylbenzene'\n",
      " 'p-Isopropyltoluene' '1,3-Dichlorobenzene' '1,4-Dichlorobenzene'\n",
      " 'n-Butylbenzene' '1,2-Dichlorobenzene' '1,2-Dibromo-3-chloropropane'\n",
      " '1,2,4-Trichlorobenzene' 'Hexachlorobutadiene' 'Naphtalene'\n",
      " '1,2,3-Trichlorobenzene' 'Conductivite in situ 20°C' 'pH mesure in situ'\n",
      " '2,6-Dichlorobenzamide' 'Metazachlor' 'Napropamide' 'Tebutam'\n",
      " 'Pesticides par GC-MS' 'Dichlobenil' 'Molinate' 'Hexachlorobenzene'\n",
      " 'Diazinon' 'Heptachlor' 'Delta-HCH' 'Ametryne' 'Aldrine' 'Parathion'\n",
      " 'Heptachlor epoxyde' \"2,4'-DDE\" \"4,4'-DDE\" 'Dieldrine' \"2,4'-DDD\"\n",
      " 'Endrine' \"2,4'-DDT\" \"4,4'-DDD\" \"4,4'-DDT\" 'Methoxychlor' 'Alachlore'\n",
      " 'Metholachlor']\n",
      "['2012-04-10T00:00:00.000000000' '2012-04-11T00:00:00.000000000'\n",
      " '2013-02-05T00:00:00.000000000' '2013-03-04T00:00:00.000000000'\n",
      " '2014-08-04T00:00:00.000000000' '2014-09-01T00:00:00.000000000'\n",
      " '2014-08-05T00:00:00.000000000' '2014-05-19T00:00:00.000000000'\n",
      " '2012-07-02T00:00:00.000000000' '2012-01-16T00:00:00.000000000'\n",
      " '2012-11-06T00:00:00.000000000' '2012-04-03T00:00:00.000000000'\n",
      " '2014-10-06T00:00:00.000000000' '2014-09-06T00:00:00.000000000'\n",
      " '2015-01-05T00:00:00.000000000' '2016-04-19T00:00:00.000000000'\n",
      " '2016-06-15T00:00:00.000000000' '2016-07-05T00:00:00.000000000'\n",
      " '2013-09-16T00:00:00.000000000' '2013-07-08T00:00:00.000000000'\n",
      " '2013-07-15T00:00:00.000000000' '2013-02-25T00:00:00.000000000'\n",
      " '2014-07-14T00:00:00.000000000' '2014-07-21T00:00:00.000000000'\n",
      " '2014-07-10T00:00:00.000000000' '2014-07-08T00:00:00.000000000'\n",
      " '2014-02-26T00:00:00.000000000' '2015-02-10T00:00:00.000000000'\n",
      " '2013-05-06T00:00:00.000000000' '2012-05-30T00:00:00.000000000'\n",
      " '2012-10-10T00:00:00.000000000' '2012-08-13T00:00:00.000000000'\n",
      " '2012-02-07T00:00:00.000000000' '2014-10-07T00:00:00.000000000'\n",
      " '2014-03-11T00:00:00.000000000' '2016-03-21T00:00:00.000000000'\n",
      " '2012-10-09T00:00:00.000000000' '2013-07-29T00:00:00.000000000'\n",
      " '2013-01-14T00:00:00.000000000' '2013-12-09T00:00:00.000000000'\n",
      " '2013-07-02T00:00:00.000000000' '2013-04-09T00:00:00.000000000'\n",
      " '2013-10-07T00:00:00.000000000' '2013-09-02T00:00:00.000000000'\n",
      " '2013-06-03T00:00:00.000000000' '2014-01-14T00:00:00.000000000'\n",
      " '2015-06-29T00:00:00.000000000' '2013-05-13T00:00:00.000000000'\n",
      " '2012-02-28T00:00:00.000000000' '2012-10-08T00:00:00.000000000'\n",
      " '2012-05-08T00:00:00.000000000' '2012-12-03T00:00:00.000000000'\n",
      " '2012-06-05T00:00:00.000000000' '2014-05-05T00:00:00.000000000'\n",
      " '2014-10-20T00:00:00.000000000' '2014-11-18T00:00:00.000000000'\n",
      " '2014-10-22T00:00:00.000000000' '2014-11-12T00:00:00.000000000'\n",
      " '2014-11-05T00:00:00.000000000' '2014-10-21T00:00:00.000000000'\n",
      " '2014-10-30T00:00:00.000000000' '2014-11-17T00:00:00.000000000'\n",
      " '2014-07-07T00:00:00.000000000' '2012-08-06T00:00:00.000000000'\n",
      " '2012-09-04T00:00:00.000000000' '2016-06-13T00:00:00.000000000'\n",
      " '2016-05-24T00:00:00.000000000' '2012-08-21T00:00:00.000000000'\n",
      " '2012-08-20T00:00:00.000000000' '2012-09-24T00:00:00.000000000'\n",
      " '2012-09-03T00:00:00.000000000' '2013-09-09T00:00:00.000000000'\n",
      " '2013-09-17T00:00:00.000000000' '2013-09-18T00:00:00.000000000'\n",
      " '2013-10-04T00:00:00.000000000' '2013-09-24T00:00:00.000000000'\n",
      " '2013-07-01T00:00:00.000000000' '2013-08-12T00:00:00.000000000'\n",
      " '2013-12-02T00:00:00.000000000' '2013-05-27T00:00:00.000000000'\n",
      " '2013-05-29T00:00:00.000000000' '2013-05-30T00:00:00.000000000'\n",
      " '2013-06-07T00:00:00.000000000' '2013-06-06T00:00:00.000000000'\n",
      " '2013-06-04T00:00:00.000000000' '2013-01-28T00:00:00.000000000'\n",
      " '2013-03-25T00:00:00.000000000' '2013-10-21T00:00:00.000000000'\n",
      " '2014-12-08T00:00:00.000000000' '2014-02-10T00:00:00.000000000'\n",
      " '2014-08-11T00:00:00.000000000' '2014-01-13T00:00:00.000000000'\n",
      " '2014-10-13T00:00:00.000000000' '2014-03-10T00:00:00.000000000'\n",
      " '2014-09-08T00:00:00.000000000' '2014-08-08T00:00:00.000000000'\n",
      " '2014-10-02T00:00:00.000000000' '2014-09-12T00:00:00.000000000'\n",
      " '2014-09-25T00:00:00.000000000' '2014-10-01T00:00:00.000000000'\n",
      " '2014-10-08T00:00:00.000000000' '2014-09-19T00:00:00.000000000'\n",
      " '2014-09-17T00:00:00.000000000' '2014-09-18T00:00:00.000000000'\n",
      " '2014-09-16T00:00:00.000000000' '2014-08-09T00:00:00.000000000'\n",
      " '2014-11-10T00:00:00.000000000' '2014-11-11T00:00:00.000000000'\n",
      " '2013-04-23T00:00:00.000000000' '2015-02-09T00:00:00.000000000'\n",
      " '2015-04-13T00:00:00.000000000' '2015-01-12T00:00:00.000000000'\n",
      " '2015-06-08T00:00:00.000000000' '2012-11-19T00:00:00.000000000'\n",
      " '2012-11-21T00:00:00.000000000' '2012-11-23T00:00:00.000000000'\n",
      " '2012-11-29T00:00:00.000000000' '2012-11-27T00:00:00.000000000'\n",
      " '2012-11-26T00:00:00.000000000' '2012-08-27T00:00:00.000000000'\n",
      " '2012-03-05T00:00:00.000000000' '2012-10-22T00:00:00.000000000'\n",
      " '2012-01-30T00:00:00.000000000' '2012-12-17T00:00:00.000000000'\n",
      " '2012-04-16T00:00:00.000000000' '2012-05-29T00:00:00.000000000'\n",
      " '2016-02-15T00:00:00.000000000' '2016-05-09T00:00:00.000000000'\n",
      " '2016-05-20T00:00:00.000000000' '2016-05-11T00:00:00.000000000'\n",
      " '2016-05-12T00:00:00.000000000' '2016-05-13T00:00:00.000000000'\n",
      " '2016-03-14T00:00:00.000000000' '2016-10-17T00:00:00.000000000'\n",
      " '2016-08-22T00:00:00.000000000' '2016-04-11T00:00:00.000000000'\n",
      " '2016-07-11T00:00:00.000000000' '2016-09-12T00:00:00.000000000'\n",
      " '2016-09-14T00:00:00.000000000' '2016-09-22T00:00:00.000000000'\n",
      " '2016-09-13T00:00:00.000000000' '2016-09-21T00:00:00.000000000'\n",
      " '2013-10-08T00:00:00.000000000' '2013-01-22T00:00:00.000000000'\n",
      " '2013-01-23T00:00:00.000000000' '2013-08-13T00:00:00.000000000'\n",
      " '2013-02-18T00:00:00.000000000' '2013-02-19T00:00:00.000000000'\n",
      " '2013-04-08T00:00:00.000000000' '2013-07-09T00:00:00.000000000'\n",
      " '2013-05-14T00:00:00.000000000' '2013-05-15T00:00:00.000000000'\n",
      " '2013-05-16T00:00:00.000000000' '2013-05-17T00:00:00.000000000'\n",
      " '2013-05-21T00:00:00.000000000' '2013-05-24T00:00:00.000000000'\n",
      " '2013-05-31T00:00:00.000000000' '2013-05-23T00:00:00.000000000'\n",
      " '2013-09-10T00:00:00.000000000' '2013-06-11T00:00:00.000000000'\n",
      " '2013-06-12T00:00:00.000000000' '2013-11-11T00:00:00.000000000'\n",
      " '2013-11-12T00:00:00.000000000' '2013-12-10T00:00:00.000000000'\n",
      " '2013-10-09T00:00:00.000000000' '2013-10-28T00:00:00.000000000'\n",
      " '2013-10-14T00:00:00.000000000' '2013-10-25T00:00:00.000000000'\n",
      " '2013-10-15T00:00:00.000000000' '2013-03-12T00:00:00.000000000'\n",
      " '2013-03-13T00:00:00.000000000' '2014-05-12T00:00:00.000000000'\n",
      " '2014-05-13T00:00:00.000000000' '2014-05-14T00:00:00.000000000'\n",
      " '2014-05-15T00:00:00.000000000' '2012-05-15T00:00:00.000000000'\n",
      " '2012-10-23T00:00:00.000000000' '2012-10-24T00:00:00.000000000'\n",
      " '2012-10-29T00:00:00.000000000' '2012-10-25T00:00:00.000000000'\n",
      " '2012-10-30T00:00:00.000000000' '2012-08-28T00:00:00.000000000'\n",
      " '2012-07-16T00:00:00.000000000' '2012-07-17T00:00:00.000000000'\n",
      " '2012-07-18T00:00:00.000000000' '2012-07-19T00:00:00.000000000'\n",
      " '2012-11-09T00:00:00.000000000' '2012-02-13T00:00:00.000000000'\n",
      " '2012-03-12T00:00:00.000000000' '2012-03-13T00:00:00.000000000'\n",
      " '2012-05-16T00:00:00.000000000' '2012-06-08T00:00:00.000000000'\n",
      " '2012-06-07T00:00:00.000000000' '2012-06-25T00:00:00.000000000'\n",
      " '2012-05-23T00:00:00.000000000' '2012-05-24T00:00:00.000000000'\n",
      " '2012-05-25T00:00:00.000000000' '2012-06-11T00:00:00.000000000'\n",
      " '2016-06-30T00:00:00.000000000' '2016-06-17T00:00:00.000000000'\n",
      " '2016-06-20T00:00:00.000000000' '2016-06-14T00:00:00.000000000'\n",
      " '2014-08-21T00:00:00.000000000' '2014-08-06T00:00:00.000000000'\n",
      " '2014-08-12T00:00:00.000000000' '2014-08-22T00:00:00.000000000'\n",
      " '2014-08-20T00:00:00.000000000' '2012-10-11T00:00:00.000000000'\n",
      " '2012-10-16T00:00:00.000000000' '2012-10-12T00:00:00.000000000'\n",
      " '2012-10-15T00:00:00.000000000' '2016-06-29T00:00:00.000000000'\n",
      " '2016-03-24T00:00:00.000000000' '2016-03-15T00:00:00.000000000'\n",
      " '2016-03-16T00:00:00.000000000' '2016-04-05T00:00:00.000000000'\n",
      " '2016-04-04T00:00:00.000000000' '2016-04-01T00:00:00.000000000'\n",
      " '2016-04-21T00:00:00.000000000' '2013-06-17T00:00:00.000000000'\n",
      " '2013-08-22T00:00:00.000000000' '2013-08-15T00:00:00.000000000'\n",
      " '2013-06-14T00:00:00.000000000' '2013-08-07T00:00:00.000000000'\n",
      " '2013-12-17T00:00:00.000000000' '2013-12-03T00:00:00.000000000'\n",
      " '2013-02-04T00:00:00.000000000' '2013-11-19T00:00:00.000000000'\n",
      " '2013-04-02T00:00:00.000000000' '2013-07-30T00:00:00.000000000'\n",
      " '2014-11-24T00:00:00.000000000' '2014-11-25T00:00:00.000000000'\n",
      " '2014-09-30T00:00:00.000000000' '2014-05-20T00:00:00.000000000'\n",
      " '2014-03-03T00:00:00.000000000' '2014-02-17T00:00:00.000000000'\n",
      " '2014-06-24T00:00:00.000000000' '2013-05-07T00:00:00.000000000'\n",
      " '2015-04-07T00:00:00.000000000' '2012-07-31T00:00:00.000000000'\n",
      " '2012-05-14T00:00:00.000000000' '2012-01-17T00:00:00.000000000'\n",
      " '2012-09-11T00:00:00.000000000' '2012-02-21T00:00:00.000000000'\n",
      " '2012-04-02T00:00:00.000000000' '2012-06-26T00:00:00.000000000'\n",
      " '2012-03-19T00:00:00.000000000' '2016-06-08T00:00:00.000000000'\n",
      " '2016-05-10T00:00:00.000000000' '2016-06-21T00:00:00.000000000'\n",
      " '2013-10-01T00:00:00.000000000' '2013-11-05T00:00:00.000000000'\n",
      " '2013-10-02T00:00:00.000000000' '2013-01-03T00:00:00.000000000'\n",
      " '2013-10-03T00:00:00.000000000' '2013-08-06T00:00:00.000000000'\n",
      " '2013-04-10T00:00:00.000000000' '2013-04-11T00:00:00.000000000'\n",
      " '2013-04-12T00:00:00.000000000' '2013-05-22T00:00:00.000000000'\n",
      " '2013-04-19T00:00:00.000000000' '2013-03-05T00:00:00.000000000'\n",
      " '2014-01-07T00:00:00.000000000' '2014-12-01T00:00:00.000000000'\n",
      " '2014-12-02T00:00:00.000000000' '2014-11-03T00:00:00.000000000'\n",
      " '2014-03-31T00:00:00.000000000' '2014-04-01T00:00:00.000000000'\n",
      " '2014-04-04T00:00:00.000000000' '2014-04-03T00:00:00.000000000'\n",
      " '2014-02-04T00:00:00.000000000' '2014-09-02T00:00:00.000000000'\n",
      " '2014-10-09T00:00:00.000000000' '2014-10-31T00:00:00.000000000'\n",
      " '2014-06-10T00:00:00.000000000' '2015-03-03T00:00:00.000000000'\n",
      " '2015-08-31T00:00:00.000000000' '2015-11-02T00:00:00.000000000'\n",
      " '2015-11-05T00:00:00.000000000' '2015-11-03T00:00:00.000000000'\n",
      " '2015-11-04T00:00:00.000000000' '2012-07-12T00:00:00.000000000'\n",
      " '2012-02-06T00:00:00.000000000' '2012-01-09T00:00:00.000000000'\n",
      " '2012-06-04T00:00:00.000000000' '2012-11-05T00:00:00.000000000'\n",
      " '2012-04-05T00:00:00.000000000' '2012-04-18T00:00:00.000000000'\n",
      " '2012-05-07T00:00:00.000000000' '2012-04-12T00:00:00.000000000'\n",
      " '2012-04-13T00:00:00.000000000' '2016-04-24T00:00:00.000000000'\n",
      " '2016-04-13T00:00:00.000000000' '2013-04-15T00:00:00.000000000'\n",
      " '2016-04-20T00:00:00.000000000' '2016-04-12T00:00:00.000000000'\n",
      " '2022-04-01T00:00:00.000000000' '2016-04-15T00:00:00.000000000'\n",
      " '2016-04-22T00:00:00.000000000' '2016-08-16T00:00:00.000000000'\n",
      " '2016-05-17T00:00:00.000000000' '2013-12-11T00:00:00.000000000'\n",
      " '2013-12-12T00:00:00.000000000' '2013-01-21T00:00:00.000000000'\n",
      " '2013-03-11T00:00:00.000000000' '2013-03-14T00:00:00.000000000'\n",
      " '2013-03-20T00:00:00.000000000' '2013-03-26T00:00:00.000000000'\n",
      " '2014-01-20T00:00:00.000000000' '2012-03-23T00:00:00.000000000'\n",
      " '2012-03-30T00:00:00.000000000' '2012-03-26T00:00:00.000000000'\n",
      " '2012-05-21T00:00:00.000000000' '2016-02-01T00:00:00.000000000'\n",
      " '2016-02-08T00:00:00.000000000' '2016-02-17T00:00:00.000000000'\n",
      " '2016-02-05T00:00:00.000000000' '2016-02-02T00:00:00.000000000'\n",
      " '2016-02-04T00:00:00.000000000' '2021-02-01T00:00:00.000000000'\n",
      " '2016-09-19T00:00:00.000000000' '2016-08-04T00:00:00.000000000'\n",
      " '2016-05-23T00:00:00.000000000' '2016-08-02T00:00:00.000000000'\n",
      " '2016-08-24T00:00:00.000000000' '2016-08-05T00:00:00.000000000'\n",
      " '2016-08-11T00:00:00.000000000' '2016-09-07T00:00:00.000000000'\n",
      " '2016-08-03T00:00:00.000000000' '2016-09-20T00:00:00.000000000'\n",
      " '2021-09-19T00:00:00.000000000' '2013-07-11T00:00:00.000000000'\n",
      " '2013-09-23T00:00:00.000000000' '2013-09-27T00:00:00.000000000'\n",
      " '2013-09-26T00:00:00.000000000' '2013-03-09T00:00:00.000000000'\n",
      " '2012-12-10T00:00:00.000000000' '2012-07-09T00:00:00.000000000'\n",
      " '2013-09-03T00:00:00.000000000' '2013-03-07T00:00:00.000000000'\n",
      " '2013-03-08T00:00:00.000000000' '2014-03-04T00:00:00.000000000'\n",
      " '2014-03-06T00:00:00.000000000' '2014-03-19T00:00:00.000000000'\n",
      " '2014-03-07T00:00:00.000000000' '2015-06-02T00:00:00.000000000'\n",
      " '2012-12-04T00:00:00.000000000' '2012-12-05T00:00:00.000000000'\n",
      " '2012-09-17T00:00:00.000000000' '2012-03-06T00:00:00.000000000'\n",
      " '2012-03-07T00:00:00.000000000' '2012-03-09T00:00:00.000000000'\n",
      " '2012-03-14T00:00:00.000000000' '2012-03-27T00:00:00.000000000'\n",
      " '2012-03-08T00:00:00.000000000' '2012-03-16T00:00:00.000000000'\n",
      " '2016-10-04T00:00:00.000000000' '2016-03-08T00:00:00.000000000'\n",
      " '2016-03-09T00:00:00.000000000' '2016-03-11T00:00:00.000000000'\n",
      " '2016-03-18T00:00:00.000000000' '2016-03-31T00:00:00.000000000'\n",
      " '2013-11-14T00:00:00.000000000' '2013-11-22T00:00:00.000000000'\n",
      " '2013-11-18T00:00:00.000000000' '2013-11-15T00:00:00.000000000'\n",
      " '2014-03-14T00:00:00.000000000' '2014-02-11T00:00:00.000000000'\n",
      " '2014-02-12T00:00:00.000000000' '2014-02-14T00:00:00.000000000'\n",
      " '2014-02-13T00:00:00.000000000' '2015-01-13T00:00:00.000000000'\n",
      " '2015-04-14T00:00:00.000000000' '2015-04-15T00:00:00.000000000'\n",
      " '2015-02-11T00:00:00.000000000' '2016-03-22T00:00:00.000000000'\n",
      " '2016-03-23T00:00:00.000000000' '2016-04-06T00:00:00.000000000'\n",
      " '2016-04-14T00:00:00.000000000' '2016-06-27T00:00:00.000000000'\n",
      " '2016-06-26T00:00:00.000000000' '2016-08-29T00:00:00.000000000'\n",
      " '2016-09-26T00:00:00.000000000' '2016-07-19T00:00:00.000000000'\n",
      " '2016-04-26T00:00:00.000000000' '2014-09-04T00:00:00.000000000'\n",
      " '2016-03-10T00:00:00.000000000' '2016-03-04T00:00:00.000000000'\n",
      " '2016-03-17T00:00:00.000000000' '2016-02-11T00:00:00.000000000'\n",
      " '2016-06-07T00:00:00.000000000' '2016-06-10T00:00:00.000000000'\n",
      " '2016-08-18T00:00:00.000000000' '2016-07-12T00:00:00.000000000'\n",
      " '1970-12-01T00:00:00.000000000' '2016-10-11T00:00:00.000000000'\n",
      " '2016-05-31T00:00:00.000000000' '2016-07-21T00:00:00.000000000'\n",
      " '2014-12-10T00:00:00.000000000' '2013-05-08T00:00:00.000000000'\n",
      " '2013-05-28T00:00:00.000000000' '2015-05-18T00:00:00.000000000'\n",
      " '2015-06-09T00:00:00.000000000' '2015-05-19T00:00:00.000000000'\n",
      " '2015-05-28T00:00:00.000000000' '2015-05-22T00:00:00.000000000'\n",
      " '2015-08-18T00:00:00.000000000' '2012-06-12T00:00:00.000000000'\n",
      " '2012-06-20T00:00:00.000000000' '2013-06-24T00:00:00.000000000'\n",
      " '2013-02-09T00:00:00.000000000' '2015-06-22T00:00:00.000000000'\n",
      " '2011-12-19T00:00:00.000000000' '2012-12-18T00:00:00.000000000'\n",
      " '2012-03-28T00:00:00.000000000' '2016-03-13T00:00:00.000000000'\n",
      " '2013-07-03T00:00:00.000000000' '2013-10-22T00:00:00.000000000'\n",
      " '2013-07-04T00:00:00.000000000' '2013-10-23T00:00:00.000000000'\n",
      " '2013-11-04T00:00:00.000000000' '2013-11-07T00:00:00.000000000'\n",
      " '2013-10-31T00:00:00.000000000' '2013-10-24T00:00:00.000000000'\n",
      " '2014-10-03T00:00:00.000000000' '2014-10-24T00:00:00.000000000'\n",
      " '2015-03-17T00:00:00.000000000' '2016-10-19T00:00:00.000000000'\n",
      " '2016-10-28T00:00:00.000000000' '2016-10-24T00:00:00.000000000'\n",
      " '2016-10-18T00:00:00.000000000' '2016-07-17T00:00:00.000000000'\n",
      " '2016-06-09T00:00:00.000000000' '2013-09-12T00:00:00.000000000'\n",
      " '2013-04-22T00:00:00.000000000' '2013-02-20T00:00:00.000000000'\n",
      " '2013-11-25T00:00:00.000000000' '2013-06-25T00:00:00.000000000'\n",
      " '2013-06-28T00:00:00.000000000' '2013-06-26T00:00:00.000000000'\n",
      " '2014-03-12T00:00:00.000000000' '2014-03-13T00:00:00.000000000'\n",
      " '2014-03-21T00:00:00.000000000' '2014-03-17T00:00:00.000000000'\n",
      " '2014-01-15T00:00:00.000000000' '2014-12-15T00:00:00.000000000'\n",
      " '2014-12-16T00:00:00.000000000' '2015-10-05T00:00:00.000000000'\n",
      " '2015-10-12T00:00:00.000000000' '2015-11-24T00:00:00.000000000'\n",
      " '2015-03-23T00:00:00.000000000' '2015-01-27T00:00:00.000000000'\n",
      " '2012-09-10T00:00:00.000000000' '2012-04-24T00:00:00.000000000'\n",
      " '2012-03-20T00:00:00.000000000' '2012-03-21T00:00:00.000000000'\n",
      " '2041-03-01T00:00:00.000000000' '2016-07-06T00:00:00.000000000'\n",
      " '2016-06-16T00:00:00.000000000' '2016-10-12T00:00:00.000000000'\n",
      " '2021-07-11T00:00:00.000000000' '2013-10-29T00:00:00.000000000'\n",
      " '2013-10-30T00:00:00.000000000' '2013-11-08T00:00:00.000000000'\n",
      " '2013-11-06T00:00:00.000000000' '2014-04-14T00:00:00.000000000'\n",
      " '2014-04-16T00:00:00.000000000' '2014-04-22T00:00:00.000000000'\n",
      " '2015-10-20T00:00:00.000000000' '2015-10-27T00:00:00.000000000'\n",
      " '2015-10-22T00:00:00.000000000' '2015-10-21T00:00:00.000000000'\n",
      " '2015-10-23T00:00:00.000000000' '2015-10-28T00:00:00.000000000'\n",
      " '2012-10-18T00:00:00.000000000' '2012-10-17T00:00:00.000000000'\n",
      " '2012-02-09T00:00:00.000000000' '2012-02-10T00:00:00.000000000'\n",
      " '2012-02-20T00:00:00.000000000' '2012-02-15T00:00:00.000000000'\n",
      " '2012-03-15T00:00:00.000000000' '2016-10-26T00:00:00.000000000'\n",
      " '2016-04-25T00:00:00.000000000' '2012-01-10T00:00:00.000000000'\n",
      " '2012-10-31T00:00:00.000000000' '2012-02-14T00:00:00.000000000'\n",
      " '2012-05-02T00:00:00.000000000' '2012-02-23T00:00:00.000000000'\n",
      " '2012-02-24T00:00:00.000000000' '2012-05-03T00:00:00.000000000'\n",
      " '2012-05-04T00:00:00.000000000' '2012-05-22T00:00:00.000000000'\n",
      " '2012-11-01T00:00:00.000000000' '2012-11-02T00:00:00.000000000'\n",
      " '2012-11-07T00:00:00.000000000' '2012-11-12T00:00:00.000000000'\n",
      " '2012-01-19T00:00:00.000000000' '2012-07-13T00:00:00.000000000'\n",
      " '2012-06-14T00:00:00.000000000' '2012-06-15T00:00:00.000000000'\n",
      " '2012-06-19T00:00:00.000000000' '2016-06-06T00:00:00.000000000'\n",
      " '2014-09-29T00:00:00.000000000' '2014-10-27T00:00:00.000000000'\n",
      " '2012-12-07T00:00:00.000000000' '2012-12-13T00:00:00.000000000'\n",
      " '2012-12-06T00:00:00.000000000' '2016-02-29T00:00:00.000000000'\n",
      " '2016-08-28T00:00:00.000000000' '2016-03-01T00:00:00.000000000'\n",
      " '2015-06-05T00:00:00.000000000' '2016-09-05T00:00:00.000000000'\n",
      " '2016-09-06T00:00:00.000000000' '2016-09-09T00:00:00.000000000'\n",
      " '2013-06-10T00:00:00.000000000' '2013-08-19T00:00:00.000000000'\n",
      " '2014-02-19T00:00:00.000000000' '2015-07-13T00:00:00.000000000'\n",
      " '2012-02-27T00:00:00.000000000' '2012-10-07T00:00:00.000000000'\n",
      " '2013-01-08T00:00:00.000000000' '2013-01-09T00:00:00.000000000'\n",
      " '2012-02-08T00:00:00.000000000' '2013-06-18T00:00:00.000000000'\n",
      " '2013-06-19T00:00:00.000000000' '2013-06-20T00:00:00.000000000'\n",
      " '2013-04-16T00:00:00.000000000' '2013-10-10T00:00:00.000000000'\n",
      " '2014-02-03T00:00:00.000000000' '2016-09-27T00:00:00.000000000'\n",
      " '2016-09-30T00:00:00.000000000' '2012-09-06T00:00:00.000000000'\n",
      " '2012-06-06T00:00:00.000000000' '2013-03-18T00:00:00.000000000'\n",
      " '2013-12-16T00:00:00.000000000' '2014-12-17T00:00:00.000000000'\n",
      " '2014-03-18T00:00:00.000000000' '2014-11-06T00:00:00.000000000'\n",
      " '2015-01-19T00:00:00.000000000' '2012-06-18T00:00:00.000000000'\n",
      " '2012-07-11T00:00:00.000000000' '2012-06-22T00:00:00.000000000'\n",
      " '2016-10-25T00:00:00.000000000' '2016-08-20T00:00:00.000000000'\n",
      " '2013-07-16T00:00:00.000000000' '2013-04-29T00:00:00.000000000'\n",
      " '2012-07-10T00:00:00.000000000' '2012-06-27T00:00:00.000000000'\n",
      " '2012-06-28T00:00:00.000000000' '2015-03-30T00:00:00.000000000'\n",
      " '2015-04-01T00:00:00.000000000' '2015-04-08T00:00:00.000000000'\n",
      " '2015-04-10T00:00:00.000000000' '2015-04-02T00:00:00.000000000'\n",
      " '2015-03-31T00:00:00.000000000' '2012-05-09T00:00:00.000000000'\n",
      " '2012-10-02T00:00:00.000000000' '2012-10-03T00:00:00.000000000'\n",
      " '2012-07-03T00:00:00.000000000' '2012-07-04T00:00:00.000000000'\n",
      " '2012-06-13T00:00:00.000000000' '2016-05-18T00:00:00.000000000'\n",
      " '2016-05-16T00:00:00.000000000' '2013-06-21T00:00:00.000000000'\n",
      " '1999-01-01T00:00:00.000000000']\n",
      "[9.990e+02 0.000e+00 9.000e+00 1.400e+01 8.000e-02 1.100e-01 7.000e+00\n",
      " 1.100e+01 6.000e-02 1.000e-01 2.000e+00 1.200e+01 9.000e-02 1.200e-01\n",
      " 1.500e+01 1.300e-01 2.500e+01 1.000e+00 1.700e+01 1.900e-01 2.100e+01\n",
      " 4.000e-02 4.000e+00 5.000e+00 3.000e+00 3.000e+01 1.010e+02 1.000e+01\n",
      " 6.000e+00 1.600e+01 8.000e+00 6.460e+02 6.100e+00 1.300e+02 2.900e+00\n",
      " 2.100e-02 1.500e-02 3.700e-02 7.600e+00 1.310e+01 1.480e+01 1.800e+01\n",
      " 4.800e+01 1.900e+01 9.200e+01 5.830e+02 1.900e+00 6.700e+00 1.649e+02\n",
      " 3.400e+00 1.230e+01 1.580e+01 1.710e+01 5.000e+01 1.000e+03 7.690e+00\n",
      " 3.870e+02 6.000e-01 1.000e-02 4.000e-01 1.600e+00 1.980e+01 9.000e-01\n",
      " 7.930e+01 2.400e+00 4.000e-03 7.100e+00 6.900e+00 4.170e+02 1.300e+00\n",
      " 8.660e+01 3.900e-02 1.100e+00 9.200e+00 8.300e+00 2.800e+01 7.710e+00\n",
      " 4.180e+02 1.900e-02 1.700e+00 2.070e+01 8.450e+01 2.600e+00 7.300e+00\n",
      " 7.400e+00 7.500e+00 3.820e+02 7.100e-01 1.100e-02 7.000e-01 1.500e+00\n",
      " 7.900e+01 2.500e+00 7.000e-03 4.270e+02 3.500e+00 2.200e-02 2.000e+01\n",
      " 1.200e+00 8.600e+01 2.700e+00 8.400e+00 1.300e+01 7.360e+00 4.660e+02\n",
      " 3.000e-01 2.000e-02 2.300e+00 2.240e+01 2.530e+01 3.100e+00 9.470e+01\n",
      " 9.100e+00 4.740e+02 3.300e+00 9.960e+01 4.600e+00 9.600e+00 9.700e+00\n",
      " 4.760e+02 9.760e+01 4.200e+00 5.000e-03 9.400e+00 7.320e+00 4.690e+02\n",
      " 2.390e+01 2.600e+01 1.800e+00 9.740e+01 4.100e+00 2.000e-03 4.300e+00\n",
      " 9.500e+00 4.970e+02 3.200e+00 1.000e+02 1.030e+01 7.740e+01 6.200e+00\n",
      " 4.950e+02 1.400e+00 9.600e+01 3.900e+00 1.010e+01 6.600e+01 2.200e+00\n",
      " 2.250e+01 2.410e+01 8.980e+01 7.350e+00 4.670e+02 4.500e+00 8.900e+00\n",
      " 8.100e+00 3.600e-01 2.800e+00 1.990e+01 7.550e+01 6.000e-03 6.400e+00\n",
      " 1.120e+01 2.950e+02 1.460e+01 2.380e+01 9.420e+01 1.600e-02 3.600e+00\n",
      " 1.140e+01 7.850e+00 6.240e+02 2.000e-01 1.800e-02 5.000e-01 3.130e+01\n",
      " 3.430e+01 1.330e+02 1.260e+01 1.180e+01 7.200e+00 6.080e+02 2.400e-01\n",
      " 2.980e+01 1.249e+02 2.100e+00 2.200e+01 4.450e+02 3.300e-02 2.270e+01\n",
      " 8.710e+01 3.700e+00 3.000e-02 6.300e+00 4.300e-02 2.340e+01 9.660e+01\n",
      " 4.400e+00 1.170e+01 6.030e+02 2.960e+01 4.500e-01 1.226e+02 1.630e+01\n",
      " 1.970e+01 7.120e+00 5.910e+02 2.840e+01 3.010e+01 4.700e+00 1.155e+02\n",
      " 8.500e+00 1.150e+01 1.070e+01 5.200e+00 1.230e+02 1.360e+01 1.680e+01\n",
      " 6.130e+02 1.310e+02 1.240e+01 2.160e+01 7.220e+00 5.980e+02 1.300e-02\n",
      " 2.830e+01 2.880e+01 5.100e+00 1.118e+02 7.000e-02 9.900e+00 1.320e+01\n",
      " 1.400e-01 8.800e+00 5.550e+02 1.700e-02 2.670e+01 2.940e+01 1.135e+02\n",
      " 2.080e+01 5.380e+02 1.700e-01 2.890e+01 5.500e+00 1.110e+02 1.190e+01\n",
      " 1.730e+01 1.340e+02 5.000e-02 7.450e+00 4.850e+02 2.290e+01 5.600e+00\n",
      " 8.600e+00 1.520e+01 7.070e+00 5.280e+02 2.460e+01 1.070e+02 1.410e+01\n",
      " 7.130e+00 5.250e+02 2.780e+01 8.000e-01 7.700e+00 1.490e+01 1.370e+01\n",
      " 4.890e+02 7.900e+00 1.020e+02 5.800e+00 4.550e+02 2.140e+01 8.220e+01\n",
      " 8.700e+00 9.000e-03 5.040e+02 2.630e+01 1.280e+01 1.330e+01 2.400e+01\n",
      " 8.550e+01 2.400e-02 5.580e+02 6.800e+00 9.510e+01 3.000e-03 3.200e-02\n",
      " 4.200e+01 5.210e+02 1.200e-02 1.610e+01 1.510e+01 4.800e+02 8.000e-03\n",
      " 5.030e+02 1.390e+01 1.380e+01 2.100e-01 2.600e-02 4.900e+00 1.210e+01\n",
      " 7.460e+00 4.960e+02 3.100e+01 2.620e+01 5.700e+00 1.160e+01 2.580e+01\n",
      " 1.290e+01 1.060e+01 1.600e-01 3.500e-02 5.600e+01 1.370e+02 4.880e+02\n",
      " 7.400e-01 2.230e+01 2.420e+01 9.430e+01 1.530e+01 2.810e+02 6.700e-01\n",
      " 2.190e+01 2.450e+01 9.320e+01 4.610e+02 1.800e-01 2.520e+01 5.300e+00\n",
      " 2.300e-01 2.800e-01 3.500e+01 3.200e-01 8.300e-02 7.420e+00 4.580e+02\n",
      " 2.610e+01 6.600e-01 9.750e+01 1.020e+01 4.620e+02 2.300e+01 2.560e+01\n",
      " 9.680e+01 4.080e+02 4.800e+00 7.370e+00 4.290e+02 2.440e+01 9.460e+01\n",
      " 7.470e+00 2.300e-02 2.130e+01 9.710e+01 4.250e+02 8.680e+01 7.310e+00\n",
      " 4.640e+02 6.300e-02 5.400e+00 2.350e+01 2.590e+01 9.910e+01 8.940e+01\n",
      " 3.100e-02 9.650e+01 4.320e+02 9.010e+01 1.110e+01 9.080e+01 5.860e+02\n",
      " 2.700e+01 1.190e+02 4.840e+02 9.400e+01 4.600e+02 9.100e-02 9.700e+01\n",
      " 7.430e+00 4.410e+02 2.120e+01 2.370e+01 8.990e+01 5.240e+02 1.030e+02\n",
      " 1.050e+01 1.200e+02 2.000e+02 5.890e+02 1.685e+02 9.300e+00 1.940e+01\n",
      " 1.100e+02 4.700e+02 1.287e+02 5.230e+02 1.515e+02 1.590e+01 7.050e+00\n",
      " 4.500e-02 2.990e+01 3.330e+01 1.290e+02 2.900e+02 7.000e+01 1.800e+02\n",
      " 6.120e+02 1.920e+01 4.400e+01 9.000e+01 4.720e+02 9.240e+01 1.080e+01\n",
      " 6.380e+02 3.030e+01 3.290e+01 6.280e+02 1.240e+02 4.070e+01 3.020e+01\n",
      " 6.210e+02 1.400e-02 2.740e+01 1.109e+02 6.600e+00 3.940e+01 7.490e+00\n",
      " 3.880e+02 1.870e+01 7.570e+01 5.900e+00 6.500e+00 7.790e+01 1.500e-01\n",
      " 3.890e+02 4.820e+02 5.340e+02 1.042e+02 6.800e+01 4.730e+02 9.500e-01\n",
      " 2.320e+01 9.410e+01 4.770e+02 2.700e-02 5.500e-01 1.750e+01 4.750e+02\n",
      " 1.032e+02 4.520e+02 2.280e+01 4.700e-01 2.210e+01 8.300e+01 1.960e+01\n",
      " 7.300e+01 4.110e+02 4.090e+02 2.020e+01 4.800e-01 8.530e+01 1.560e+01\n",
      " 4.100e+02 3.910e+02 2.600e-01 3.900e+02 4.900e-01 4.810e+02 6.300e-01\n",
      " 5.350e+02 4.030e+02 2.500e-01 4.680e+02 4.920e+02 5.130e+02 4.060e+02\n",
      " 5.110e+02 4.570e+02 4.500e+02 4.650e+02 4.040e+02 3.980e+02 3.410e+02\n",
      " 5.260e+02 3.920e+02 4.910e+02 5.460e+02 3.060e+01 9.800e+00 4.430e+02\n",
      " 7.670e+00 4.830e+02 2.220e+01 2.680e+01 6.900e-01 5.200e-01 7.600e-01\n",
      " 8.200e+00 4.900e+02 1.880e+01 8.540e+01 4.240e+02 2.200e-01 2.510e+01\n",
      " 7.170e+01 4.300e+01 4.600e+01 4.100e+01 3.600e+01 2.350e+02 2.900e+01\n",
      " 3.800e+02 7.090e+00 6.020e+02 8.100e-02 3.040e+01 1.160e+02 1.080e+02\n",
      " 4.900e+01 5.820e+02 1.120e+02 3.800e+00 1.790e+01 1.750e+02 6.040e+02\n",
      " 3.200e+01 6.200e+02 6.260e+02 1.720e+01 5.170e+02 6.100e-01 9.690e+01\n",
      " 7.210e+00 5.750e+02 1.170e+02 4.000e+01 6.070e+02 5.100e+01 8.500e+01\n",
      " 2.100e+03 9.660e+02 1.670e+02 1.670e+01 1.260e+02 1.090e+01 4.210e+02\n",
      " 1.950e+01 1.850e+01 6.810e+01 2.470e+01 3.990e+02 2.060e+01 8.160e+01\n",
      " 7.650e+00 4.390e+02 8.790e+01 8.260e+01 9.520e+01 2.820e+01 4.300e+02\n",
      " 2.550e+00 8.700e+01 8.520e+01 3.850e+00 8.210e+01 2.260e+01 7.880e+01\n",
      " 1.860e+01 4.260e+02 1.930e+01 8.020e+01 1.140e+02 7.260e+00 4.630e+02\n",
      " 9.980e+01 7.410e+00 1.150e+02 1.130e+01 4.980e+02 1.340e+01 1.780e+01\n",
      " 2.330e+01 2.550e+01 6.000e+01 2.660e+01 7.800e+00 2.540e+01 5.650e+02\n",
      " 1.050e+02 9.180e+01 9.870e+01 2.710e+01 1.206e+02 3.300e-01 4.600e-02\n",
      " 5.760e+02 3.050e+01 1.180e+02 2.930e+01 5.620e+02 1.350e+01 5.780e+02\n",
      " 5.680e+02 3.140e+01 1.220e+02 6.060e+02 2.950e+01 1.220e+01 1.700e+02\n",
      " 3.380e+02 6.720e+01 3.360e+02 1.000e-03 1.650e+01 6.790e+01 3.420e+02\n",
      " 1.310e+00 1.840e+01 6.970e+01 9.170e+01 3.400e-02 5.370e+02 1.090e+02\n",
      " 4.100e-02 5.300e+02 1.250e-01 1.540e+01 2.010e+01 4.350e+02 2.150e+01\n",
      " 8.240e+01 4.510e+02 2.310e+01 4.220e+02 6.400e-01 2.180e+01 8.800e-01\n",
      " 5.360e+02 4.790e+02 6.500e-01 1.620e+00 1.034e+02 4.300e-01 1.038e+02\n",
      " 7.400e+01 3.000e+02 8.000e+01 8.800e+01 7.180e+00 5.020e+02 2.490e+01\n",
      " 3.350e+01 1.039e+02 6.180e+02 5.950e+02 6.320e+02 3.260e+01 2.760e+01\n",
      " 1.313e+02 3.850e+01 9.800e+01 7.080e+00 5.500e+02 9.600e-02 2.810e+01\n",
      " 6.900e-02 2.190e+02 5.500e+01 5.570e+02 1.080e-01 1.810e+01 4.990e+02\n",
      " 2.200e+02 3.680e+02 1.440e-01 9.500e+01 5.390e+02 3.500e-01 5.290e+02\n",
      " 1.060e+02 9.400e-01 5.900e+01 1.500e+02 8.400e+01 9.100e+01 5.800e+01\n",
      " 8.860e+01 4.780e+02 9.380e+01 6.990e+01 3.710e+02 6.910e+01 5.070e+02\n",
      " 7.910e+00 4.010e+02 7.070e+01 5.960e+02 5.940e+02 2.510e+02 4.150e+02\n",
      " 4.480e+02 4.140e+02 7.500e+01 4.130e+02 5.970e+02 4.050e+02 5.200e+02\n",
      " 5.700e+02 4.940e+02 2.850e+02 3.490e+02 3.400e+01 1.280e+02 7.770e+02\n",
      " 3.780e+02 4.310e+02 4.470e+02 4.360e+02 5.400e+01 3.790e+02 2.800e+02\n",
      " 1.270e+02 6.300e+01 4.370e+02 5.700e+01 9.300e+01 5.990e+02 4.460e+02\n",
      " 1.820e+02 3.750e+02 1.250e+02 2.060e+02 2.017e+03 4.490e+02 2.590e+02\n",
      " 4.590e+02 2.890e+00 3.770e+02 3.290e+02 5.430e-01 5.010e+02 5.530e+02\n",
      " 8.080e-01 1.070e+00 2.900e-02 1.660e-01 3.530e+02 5.740e+02 1.680e-01\n",
      " 1.760e+02 3.810e+02 5.480e+01 4.340e+02 3.690e+02 3.640e+02 3.950e+02\n",
      " 4.280e+02 5.930e+02 5.730e+02 1.170e-01 2.190e-01 1.190e-01 8.800e-02\n",
      " 2.330e-01 5.000e+02]\n",
      "['°C' 'UFC/100ml' 'mg/l' nan 'µS/cm' 'FTU' 'cm-1' '°f' 'f' 'µg/l' 'μg/l']\n",
      "[9.99e+02 0.00e+00 3.00e+02 1.00e-01 1.00e+00 2.00e+00 5.00e-01 2.00e+02\n",
      " 2.00e-01 4.00e+01 1.00e+01]\n",
      "[  1892   1893    518 ... 201560 201561 180778]\n",
      "[  1 999   2   3   4   5]\n",
      "[ 11.7   12.7  999.    14.    11.    12.    15.    25.    17.    21.\n",
      "   7.    17.1   18.3    9.6    8.7   12.3    9.3    9.4   12.5   14.5\n",
      "  13.5   16.5    8.5   16.    18.    18.5   19.    11.5   10.     9.5\n",
      "  10.5    9.    15.5   10.2   13.    17.5   15.2   20.     8.2   15.8\n",
      "  15.3    9.2   15.7   13.3   13.2    9.9   12.2   15.1   20.5   16.4\n",
      "  13.4    7.2   13.9   12.1    9.7   16.2   15.4   14.1    7.7    8.\n",
      "   6.     5.3   12.4    6.8   18.8   10.6    4.3   14.8   18.6   20.4\n",
      "  17.4   19.8   13.1   10.1   13.6   12.8   18.2   14.6    8.8    7.1\n",
      "  11.4   11.1   17.9    7.4    8.3   17.2   13.8   12.6    9.8   10.9\n",
      "  10.7   17.7   10.8    6.7    5.     6.5    4.     7.5   19.5    8.1\n",
      "   7.9    7.25   7.3    7.8   10.3    9.1   15.9   11.6   18.9   21.5\n",
      "  20.2   22.6   21.9   13.7   11.3   14.7   16.7   22.1   18.7   11.8\n",
      "  19.9   12.9   20.1   16.8   18.4   16.9   15.6    8.9   11.2   15.03\n",
      "  19.2   19.6   17.8   19.4   16.1   16.3   18.1   19.1   20.8    3.5\n",
      "   7.6    8.4   21.3   21.7   27.9    4.5    5.5    3.    22.5    1.\n",
      "  20.6    8.6   16.6   14.4   10.4   22.4   21.1   22.9    0.    22.\n",
      "   2.     4.9    5.8 ]\n",
      "['Bassecourt, eau de réseau, Espace Industriel 30, Belidis'\n",
      " 'Bassecourt, eau de réseau, Rue abbé Monnin, FMB' 'SEHA, Réseau' ...\n",
      " 'Bassecourt. bout de réseau' 'bout de réseau, Courfaivre'\n",
      " 'Réservoir 1500 m° Z1,']\n",
      "['Eau brute' 'Eau traitee' None 'Reseau' 'Reservoir']\n",
      "['UV' 'dioxyde de chlore/UF' 'None' 'dioxyde de chlore'\n",
      " 'ozone, eau de javel' 'UF' 'ozone' 'UV, dioxyde de chlore'\n",
      " 'UV, eau de javel' 'eau de javel' 'dioxyde de chlore, eau de'\n",
      " 'UV, dioxyde de chlore,UF' 'eau de javel, UF'\n",
      " 'ultra-filtration/eau de javel' 'ultra-filtrationteau de javel'\n",
      " 'UV, ozone' 'eau de javel, UV' 'ozone, dioxyde de chlore' 'charbon actif'\n",
      " 'UF, UV' 'dioxyde de Chlore, UF' 'dioxyde de chlore, chlore'\n",
      " 'eau de secours Porrentruy' 'UV/UF' 'brute' 'Javel/UF' 'Chlore/UF'\n",
      " 'onore/UV UV' 'cores' '/chlore' 'Chlore/UV' '' 'Ozone/javel'\n",
      " 'Ozone/Javel' 'Brute' 'Chlore' 'nia' 'UF/UV' 'Ozone' 'Ozone/Chlore'\n",
      " 'Javel' 'Ozone/eau de javel' 'nan' 'de javel' 'Eau de javelE  au'\n",
      " 'Ozone/UF' 'javel' 'mg/L' 'Jave/lUF' 'Javel/UV' 'Ozone/Uavel' '/Chlore'\n",
      " 'Chlore total' 'libre QO' 'Chlore, ozone' 'Chlore/UV/UF' 'UV/Javel'\n",
      " 'UF/Chlore' 'UV/javel' 'UV/chlore' 'UV/Chlore']\n",
      "[None 'Jacques Heyer' 'Yves Salomon' 'Pierre Stieger' 'Allemann J.-P.'\n",
      " 'Luginbuhl Didier' 'Pierre-Andre Guerdat' 'Michel Chevre' 'Michel Dupre'\n",
      " 'Chapuis Serge' 'Serge Gschwind' 'Raymond Voillat' 'Joel Stalder'\n",
      " 'Jean-Pierre Pataoner' 'Francis Droz' 'Michel Wermeille'\n",
      " 'Fernando Ribeiro' 'Florent Schori' 'Andre Chappuis' 'Valli Ugo'\n",
      " 'Goudron Vincent' 'Falbriard Gabriel' 'F. G.' 'Jean-Luc Domine'\n",
      " 'Herve Gerster' 'Gregory Jeannerat' 'Marchand Jacques' 'Allemann Blaise'\n",
      " 'Fluck K.' 'Hauser Stephane' 'Panier Didier' 'Pierre Petignat'\n",
      " 'Didier Panier' 'Christian Schneider' 'Francois Hernandez' 'Corbat, C.'\n",
      " 'Corbat C.' 'Andre Vauclair' 'Ernest Monin' 'Damien Belet'\n",
      " 'Dominique Queloz' 'Alain Baume' 'Jean-Pierre Coulon' 'Gerard Coulon'\n",
      " 'Clerc Paul' 'Mathieu Grossenbacher' 'Roth M.' 'Paul Crelin'\n",
      " 'Paul Crelier' 'Serge Chapuis' 'Johann Stalder' 'Chapuis S.'\n",
      " 'Ernest Muller' 'Marc Duchene' 'Pierre Voisard' 'Patrick Sudan'\n",
      " 'Richard Chevre' 'Jose Petermann' 'Andre Joray' 'Stephane Klay'\n",
      " 'Fulgido Fabio' '' 'Gregory M.' 'S. Rufer' 'M.Rich' 'Marc Grossenbacher'\n",
      " 'Rich M' 'Chagouis Serge' 'Seuret/Valerie Julien' 'Rich M.'\n",
      " 'Grosssenbacher M' 'S.Rufer/M.Rich' 'C.Corbat' 'Fulgido F.'\n",
      " 'Fuigido Fabio' 'Rebetez Daniel' 'Vieux Christophe' 'Chapuis Sege'\n",
      " 'Muller E.' 'Muller, E.' 'Schaller 0.' 'Olivier M.' 'Fuetterer M.'\n",
      " 'Fuetterer Benoit' 'Yves M.' 'Joray M.' 'Grossenbacher Mathieu' 'S.Kklay'\n",
      " 'du Norn' 'Jeannerat Gregoy' 'Fulganc Fabio' 'Rufer B.Stettler/S' 'MRich'\n",
      " 'Fabio Fulgano' 'Bourquard. J.' 'Quiquerez Serge' 'Rich Martial'\n",
      " 'Comment Richard' 'F.Fuigido' 'Petermann M.' 'D. Monsieur' 'Queloz M.'\n",
      " 'S.Rufer/D.Queloz' 'Chapouis Andre']\n",
      "['./Data/Autocontrole_EP/Bassecourt/2012/01892.pdf'\n",
      " './Data/Autocontrole_EP/Bassecourt/2012/01893.pdf'\n",
      " './Data/Autocontrole_EP/Grandfontaine/2013/00518.pdf' ...\n",
      " './Data/Autocontrole_EP/Haute-Sorne/2018/20181116/182298-1.pdf'\n",
      " './Data/Autocontrole_EP/Cornol/2020/20200922/RCsultats_danalyses_du_07_septembre_2020.pdf'\n",
      " './Data/Autocontrole_EP/Boncourt/2018/20180427/180778_1.pdf']\n",
      "['Bassecourt' 'Grandfontaine' 'Boecourt' 'Delemont' 'Bressaucourt'\n",
      " 'Courgenay' None 'Cornol' 'Fahy' 'Movelier' 'Les_Enfers' 'Courtedoux'\n",
      " 'Vellerat' 'Courroux' 'Boncourt' 'Rocourt' 'Saignelegier' 'Mettembert'\n",
      " 'Courfaivre' 'Chatillon' 'Porrentruy' 'Bure' 'Develier' 'Miecourt'\n",
      " 'Bourrignon' 'Soulce' 'Fontenais' 'Chevenez' 'Courtetelle' 'Soyhieres'\n",
      " 'Charmoille' 'Courtemaiche' 'Courrendlin' 'Rebeuvelier' 'Vermes']\n",
      "['Haute-Sorne' 'Grandfontaine' 'Boecourt' 'Delemont' 'Fontenais'\n",
      " 'Courgenay' 'Basse-Allaine' 'Cornol' 'Fahy' 'Haute-Ajoie' 'SEHA'\n",
      " 'Movelier' 'Les_Enfers' 'Courtedoux' 'Courrendlin' 'Courroux' 'Boncourt'\n",
      " 'Saignelegier' 'SEV' 'Mettembert' 'La_Baroche' 'Chatillon' 'Porrentruy'\n",
      " 'Bure' 'Develier' 'Bourrignon' 'SEF' 'Courtetelle' 'SEVT' 'Soyhieres'\n",
      " 'Clos_du_Doubs' 'vR_production_Von_Roll' 'Val_Terbi'\n",
      " 'Syndicat_intercommunal_des_eaux_de_CourtCtelle_et_Courfaivre']\n",
      "['Conseil communal de Haute-Sorne' 'Conseil communal de Grandfontaine'\n",
      " 'Conseil communal de Boécourt' 'Conseil communal de Delémont /SID'\n",
      " 'Conseil communal de Fontenais' 'Conseil communal de Courgenay'\n",
      " 'Conseil communal de Basse-Allaine' 'Conseil communal de Cornol'\n",
      " 'Conseil communal de Fahy' 'Conseil communal de Haute-Ajoie' 'SEHA'\n",
      " 'Conseil communal de Movelier' 'Conseil communal des Enfers'\n",
      " 'Conseil communal de Courtedoux' 'Conseil communal de Courrendlin'\n",
      " 'Conseil communal de Courroux' 'Conseil communal de Boncourt'\n",
      " 'Conseil communal du Saignelégier' 'SEV' 'Conseil communal de Mettembert'\n",
      " 'Conseil communal de La Baroche' 'Conseil communal de Châtillon'\n",
      " 'Conseil communal de Porrentruy' 'Conseil communal de Bure'\n",
      " 'Conseil communal de Develier' 'Conseil communal de Bourrignon' 'SEF'\n",
      " 'Conseil communal de Courtételle' 'SEVT' 'Conseil communal de Soyhières'\n",
      " 'Clos_du_Doubs' 'vR_production_Von_Roll'\n",
      " 'Syndicat_intercommunal_des_eaux_de_CourtCtelle_et_Courfaivre']\n"
     ]
    }
   ],
   "source": [
    "for c in df.columns:\n",
    "    print(df[c].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_df_to_relational_table(df):\n",
    "    df_Analysis = pd.DataFrame(columns=col_Analysis)\n",
    "    df_Parameter = pd.DataFrame(columns=col_Parameter)\n",
    "    df_Echantillon = pd.DataFrame(columns=col_Echantillon)\n",
    "    df_Location = pd.DataFrame(columns=col_Location)\n",
    "    df_Sampler = pd.DataFrame(columns=col_Sampler)\n",
    "    \n",
    "    df_Parameter = df[[\"Parametre\", \"Unite\", \"Limit\"]].drop_duplicates().reset_index(drop=True).reset_index(drop=False)\n",
    "    df_Parameter = df_Parameter.rename(columns={\"index\": \"ID\", \"Parametre\": \"Nom\", \"Unite\": \"Unit\"})\n",
    "    df_Parameter[\"Group\"] = \"None\"\n",
    "    df_Parameter = df_Parameter[col_Parameter]\n",
    "    \n",
    "    df_Location = df[[\"Address\", \"WaterType\", \"Treatment\", \"Village\", \"Commune\", \"Distributeur\"]].drop_duplicates().reset_index(drop=True).reset_index(drop=False)\n",
    "    df_Location = df_Location.rename(columns={\"index\": \"ID\"})\n",
    "    df_Location = df_Location[col_Location]\n",
    "    \n",
    "    df_Sampler = df[[\"Sampler\"]].drop_duplicates()\n",
    "    df_Sampler[\"FirstName\"] = df_Sampler.Sampler.map(lambda x: str(x).split()[0] if x else \"None\")\n",
    "    df_Sampler[\"LastName\"] = df_Sampler.Sampler.map(lambda x: str(x).split()[1] if x and len(str(x).split())>1 else \"None\")\n",
    "    #df_Sampler = df_Sampler.drop(columns=[\"Sampler\"]) # as needed keep for instance\n",
    "    df_Sampler = df_Sampler.reset_index(drop=True).reset_index(drop=False).rename(columns={\"index\": \"ID\"})\n",
    "    \n",
    "    df_Echantillon = df[[\"Address\", \"Sampler\", \"Code\", \"FloconsNb\", \"Temperature\"]].drop_duplicates().reset_index(drop=True).reset_index(drop=False)\n",
    "    df_Echantillon = df_Echantillon.rename(columns={\"index\": \"ID\"})\n",
    "    address_dict = {v: k for k, v in df_Location.set_index(\"ID\").Address.to_dict().items()}\n",
    "    sampler_dict = {v: k for k, v in df_Sampler.set_index(\"ID\").Sampler.to_dict().items()}\n",
    "    df_Echantillon = df_Echantillon.replace({\"Address\": address_dict, \"Sampler\": sampler_dict})\n",
    "    df_Echantillon = df_Echantillon.rename(columns={\"Address\": \"AddressID\", \"Sampler\": \"SamplerID\"})\n",
    "    \n",
    "    df_Analysis = df[[\"Parametre\", \"Code\", \"Unite\", \"Limit\", \"Address\", \"Sampler\", \"Value\", \"Date\"]].drop_duplicates().reset_index(drop=True).reset_index(drop=False)\n",
    "    df_Analysis = df_Analysis.rename(columns={\"index\": \"ID\"})\n",
    "    df_Analysis = df_Analysis.replace({\"Address\": address_dict, \"Sampler\": sampler_dict})\n",
    "    df_Analysis = df_Analysis.rename(columns={\"Address\": \"AddressID\", \"Sampler\": \"SamplerID\"})\n",
    "    Echantillon_keys = [\"Code\", \"AddressID\", \"SamplerID\"]\n",
    "    df_Analysis = df_Analysis.merge(df_Echantillon[[\"ID\"]+Echantillon_keys].rename(columns={\"ID\": \"EchantillonID\"}), on=Echantillon_keys, how=\"inner\").drop(columns=Echantillon_keys)\n",
    "    Parameter_keys = [\"Nom\", \"Unit\", \"Limit\"]\n",
    "    df_Analysis = df_Analysis.rename(columns={\"Parametre\": \"Nom\", \"Unite\": \"Unit\"}).merge(df_Parameter[[\"ID\"]+Parameter_keys].rename(columns={\"ID\": \"ParamID\"}), on=Parameter_keys, how=\"inner\").drop(columns=Parameter_keys)\n",
    "    df_Analysis = df_Analysis.reset_index(drop=True)\n",
    "    \n",
    "    return df_Analysis.drop(columns=[\"ID\"]), df_Parameter.drop(columns=[\"ID\"]), df_Echantillon.drop(columns=[\"ID\"]), df_Location.drop(columns=[\"ID\"]), df_Sampler.drop(columns=[\"ID\", \"Sampler\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Analysis, df_Parameter, df_Echantillon, df_Location, df_Sampler = from_df_to_relational_table(df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for c in df_Sampler.columns:\n",
    "    print(c)\n",
    "    print(df_Sampler[c].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Analysis.to_excel(\"analysis.xlsx\")\n",
    "df_Parameter.to_excel(\"parameter.xlsx\")\n",
    "df_Echantillon.to_excel(\"echantillon.xlsx\")\n",
    "df_Location.to_excel(\"location.xlsx\")\n",
    "df_Sampler.to_excel(\"sampler.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "      <th>Date</th>\n",
       "      <th>EchantillonID</th>\n",
       "      <th>ParamID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>999.0</td>\n",
       "      <td>2012-04-10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>999.0</td>\n",
       "      <td>2012-04-10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.0</td>\n",
       "      <td>2014-10-06</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.0</td>\n",
       "      <td>2014-10-06</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.0</td>\n",
       "      <td>2014-10-06</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15312</th>\n",
       "      <td>20.0</td>\n",
       "      <td>2013-07-09</td>\n",
       "      <td>1443</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15313</th>\n",
       "      <td>20.0</td>\n",
       "      <td>2013-07-09</td>\n",
       "      <td>1444</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15314</th>\n",
       "      <td>999.0</td>\n",
       "      <td>2013-04-29</td>\n",
       "      <td>1445</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15315</th>\n",
       "      <td>50.0</td>\n",
       "      <td>2013-04-29</td>\n",
       "      <td>1445</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15316</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2013-04-29</td>\n",
       "      <td>1445</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15317 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Value       Date  EchantillonID  ParamID\n",
       "0      999.0 2012-04-10              0        0\n",
       "1      999.0 2012-04-10              1        0\n",
       "2       14.0 2014-10-06             13        0\n",
       "3       11.0 2014-10-06             14        0\n",
       "4       12.0 2014-10-06             15        0\n",
       "...      ...        ...            ...      ...\n",
       "15312   20.0 2013-07-09           1443      154\n",
       "15313   20.0 2013-07-09           1444      154\n",
       "15314  999.0 2013-04-29           1445      155\n",
       "15315   50.0 2013-04-29           1445      155\n",
       "15316   10.0 2013-04-29           1445      156\n",
       "\n",
       "[15317 rows x 4 columns]"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nom</th>\n",
       "      <th>Group</th>\n",
       "      <th>Unit</th>\n",
       "      <th>Limit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Temperature mesuree in-situ</td>\n",
       "      <td>None</td>\n",
       "      <td>°C</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>None</td>\n",
       "      <td>UFC/100ml</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Enterocoques</td>\n",
       "      <td>None</td>\n",
       "      <td>UFC/100ml</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Germes aerobies</td>\n",
       "      <td>None</td>\n",
       "      <td>UFC/100ml</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chlore libre in-situ</td>\n",
       "      <td>None</td>\n",
       "      <td>mg/l</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>4,4'-DDD</td>\n",
       "      <td>None</td>\n",
       "      <td>μg/l</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>4,4'-DDT</td>\n",
       "      <td>None</td>\n",
       "      <td>μg/l</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Methoxychlor</td>\n",
       "      <td>None</td>\n",
       "      <td>μg/l</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>Alachlore</td>\n",
       "      <td>None</td>\n",
       "      <td>μg/l</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>Metholachlor</td>\n",
       "      <td>None</td>\n",
       "      <td>μg/l</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>157 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Nom Group       Unit  Limit\n",
       "0    Temperature mesuree in-situ  None         °C  999.0\n",
       "1               Escherichia coli  None  UFC/100ml    0.0\n",
       "2                   Enterocoques  None  UFC/100ml    0.0\n",
       "3                Germes aerobies  None  UFC/100ml  300.0\n",
       "4           Chlore libre in-situ  None       mg/l    0.1\n",
       "..                           ...   ...        ...    ...\n",
       "152                     4,4'-DDD  None       μg/l    0.1\n",
       "153                     4,4'-DDT  None       μg/l    0.1\n",
       "154                 Methoxychlor  None       μg/l    0.1\n",
       "155                    Alachlore  None       μg/l    0.1\n",
       "156                 Metholachlor  None       μg/l    0.1\n",
       "\n",
       "[157 rows x 4 columns]"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AddressID</th>\n",
       "      <th>SamplerID</th>\n",
       "      <th>Code</th>\n",
       "      <th>FloconsNb</th>\n",
       "      <th>Temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1892</td>\n",
       "      <td>1</td>\n",
       "      <td>11.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1893</td>\n",
       "      <td>1</td>\n",
       "      <td>12.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "      <td>518</td>\n",
       "      <td>1</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1020</td>\n",
       "      <td>1</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2960</td>\n",
       "      <td>1</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>1817</td>\n",
       "      <td>25</td>\n",
       "      <td>193299</td>\n",
       "      <td>2</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>1818</td>\n",
       "      <td>25</td>\n",
       "      <td>182298</td>\n",
       "      <td>2</td>\n",
       "      <td>13.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>1331</td>\n",
       "      <td>42</td>\n",
       "      <td>201560</td>\n",
       "      <td>1</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>1819</td>\n",
       "      <td>42</td>\n",
       "      <td>201561</td>\n",
       "      <td>1</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>1820</td>\n",
       "      <td>12</td>\n",
       "      <td>180778</td>\n",
       "      <td>999</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      AddressID  SamplerID    Code  FloconsNb  Temperature\n",
       "0             0          0    1892          1         11.7\n",
       "1             1          0    1893          1         12.7\n",
       "2           125          0     518          1        999.0\n",
       "3             3          0    1020          1        999.0\n",
       "4             4          0    2960          1        999.0\n",
       "...         ...        ...     ...        ...          ...\n",
       "2220       1817         25  193299          2         12.0\n",
       "2221       1818         25  182298          2         13.5\n",
       "2222       1331         42  201560          1        999.0\n",
       "2223       1819         42  201561          1        999.0\n",
       "2224       1820         12  180778        999        999.0\n",
       "\n",
       "[2225 rows x 5 columns]"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Echantillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Address</th>\n",
       "      <th>WaterType</th>\n",
       "      <th>Treatment</th>\n",
       "      <th>Village</th>\n",
       "      <th>Commune</th>\n",
       "      <th>Distributeur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bassecourt, eau de réseau, Espace Industriel 3...</td>\n",
       "      <td>Eau brute</td>\n",
       "      <td>UV</td>\n",
       "      <td>Bassecourt</td>\n",
       "      <td>Haute-Sorne</td>\n",
       "      <td>Conseil communal de Haute-Sorne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bassecourt, eau de réseau, Rue abbé Monnin, FMB</td>\n",
       "      <td>Eau brute</td>\n",
       "      <td>UV</td>\n",
       "      <td>Bassecourt</td>\n",
       "      <td>Haute-Sorne</td>\n",
       "      <td>Conseil communal de Haute-Sorne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SEHA, Réseau</td>\n",
       "      <td>Eau brute</td>\n",
       "      <td>dioxyde de chlore/UF</td>\n",
       "      <td>Grandfontaine</td>\n",
       "      <td>Grandfontaine</td>\n",
       "      <td>Conseil communal de Grandfontaine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SEHA, Stap. Roche d'Or</td>\n",
       "      <td>Eau brute</td>\n",
       "      <td>dioxyde de chlore/UF</td>\n",
       "      <td>Grandfontaine</td>\n",
       "      <td>Grandfontaine</td>\n",
       "      <td>Conseil communal de Grandfontaine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Grandfontaine, Réseau</td>\n",
       "      <td>Eau traitee</td>\n",
       "      <td>None</td>\n",
       "      <td>Grandfontaine</td>\n",
       "      <td>Grandfontaine</td>\n",
       "      <td>Conseil communal de Grandfontaine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>Sceut. bout de réseau (eau du SEF]</td>\n",
       "      <td>Reseau</td>\n",
       "      <td>None</td>\n",
       "      <td>Bassecourt</td>\n",
       "      <td>Haute-Sorne</td>\n",
       "      <td>Conseil communal de Haute-Sorne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>Bassecourt. bout de réseau</td>\n",
       "      <td>Reseau</td>\n",
       "      <td>UV</td>\n",
       "      <td>Bassecourt</td>\n",
       "      <td>Haute-Sorne</td>\n",
       "      <td>Conseil communal de Haute-Sorne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>bout de réseau, Courfaivre</td>\n",
       "      <td>Reseau</td>\n",
       "      <td>UV/javel</td>\n",
       "      <td>None</td>\n",
       "      <td>Haute-Sorne</td>\n",
       "      <td>Conseil communal de Haute-Sorne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1819</th>\n",
       "      <td>Source eau brute</td>\n",
       "      <td>Eau brute</td>\n",
       "      <td>Ozone</td>\n",
       "      <td>Cornol</td>\n",
       "      <td>Cornol</td>\n",
       "      <td>Conseil communal de Cornol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820</th>\n",
       "      <td>Réservoir 1500 m° Z1,</td>\n",
       "      <td>Reservoir</td>\n",
       "      <td>None</td>\n",
       "      <td>Boncourt</td>\n",
       "      <td>Boncourt</td>\n",
       "      <td>Conseil communal de Boncourt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1821 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Address    WaterType  \\\n",
       "0     Bassecourt, eau de réseau, Espace Industriel 3...    Eau brute   \n",
       "1       Bassecourt, eau de réseau, Rue abbé Monnin, FMB    Eau brute   \n",
       "2                                          SEHA, Réseau    Eau brute   \n",
       "3                                SEHA, Stap. Roche d'Or    Eau brute   \n",
       "4                                 Grandfontaine, Réseau  Eau traitee   \n",
       "...                                                 ...          ...   \n",
       "1816                 Sceut. bout de réseau (eau du SEF]       Reseau   \n",
       "1817                         Bassecourt. bout de réseau       Reseau   \n",
       "1818                         bout de réseau, Courfaivre       Reseau   \n",
       "1819                                   Source eau brute    Eau brute   \n",
       "1820                              Réservoir 1500 m° Z1,    Reservoir   \n",
       "\n",
       "                 Treatment        Village        Commune  \\\n",
       "0                       UV     Bassecourt    Haute-Sorne   \n",
       "1                       UV     Bassecourt    Haute-Sorne   \n",
       "2     dioxyde de chlore/UF  Grandfontaine  Grandfontaine   \n",
       "3     dioxyde de chlore/UF  Grandfontaine  Grandfontaine   \n",
       "4                     None  Grandfontaine  Grandfontaine   \n",
       "...                    ...            ...            ...   \n",
       "1816                  None     Bassecourt    Haute-Sorne   \n",
       "1817                    UV     Bassecourt    Haute-Sorne   \n",
       "1818              UV/javel           None    Haute-Sorne   \n",
       "1819                 Ozone         Cornol         Cornol   \n",
       "1820                  None       Boncourt       Boncourt   \n",
       "\n",
       "                           Distributeur  \n",
       "0       Conseil communal de Haute-Sorne  \n",
       "1       Conseil communal de Haute-Sorne  \n",
       "2     Conseil communal de Grandfontaine  \n",
       "3     Conseil communal de Grandfontaine  \n",
       "4     Conseil communal de Grandfontaine  \n",
       "...                                 ...  \n",
       "1816    Conseil communal de Haute-Sorne  \n",
       "1817    Conseil communal de Haute-Sorne  \n",
       "1818    Conseil communal de Haute-Sorne  \n",
       "1819         Conseil communal de Cornol  \n",
       "1820       Conseil communal de Boncourt  \n",
       "\n",
       "[1821 rows x 6 columns]"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FirstName</th>\n",
       "      <th>LastName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jacques</td>\n",
       "      <td>Heyer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yves</td>\n",
       "      <td>Salomon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pierre</td>\n",
       "      <td>Stieger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Allemann</td>\n",
       "      <td>J.-P.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Petermann</td>\n",
       "      <td>M.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>D.</td>\n",
       "      <td>Monsieur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Queloz</td>\n",
       "      <td>M.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>S.Rufer/D.Queloz</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Chapouis</td>\n",
       "      <td>Andre</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            FirstName  LastName\n",
       "0                None      None\n",
       "1             Jacques     Heyer\n",
       "2                Yves   Salomon\n",
       "3              Pierre   Stieger\n",
       "4            Allemann     J.-P.\n",
       "..                ...       ...\n",
       "99          Petermann        M.\n",
       "100                D.  Monsieur\n",
       "101            Queloz        M.\n",
       "102  S.Rufer/D.Queloz      None\n",
       "103          Chapouis     Andre\n",
       "\n",
       "[104 rows x 2 columns]"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
